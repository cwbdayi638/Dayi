{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cwbdayi638/Dayi/blob/main/MCP_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Prug3cAQL08E"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uwv5-PeDL7Pc",
        "outputId": "d0948dd6-972e-4293-b892-42a425e0f8e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: gradio_client in /usr/local/lib/python3.11/dist-packages (1.11.1)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.178.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.14.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio_client) (2025.3.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio_client) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from gradio_client) (0.34.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio_client) (25.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio_client) (15.0.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio_client) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio_client) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio_client) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio_client) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio_client) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.3->gradio_client) (3.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.3->gradio_client) (6.0.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.3->gradio_client) (1.1.7)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.24.1->gradio_client) (1.3.1)\n",
            "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install google-generativeai gradio_client python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "TOdqlf9vMCSy",
        "outputId": "28cf0825-ec61-4622-f06e-6c368fc3fb86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ä½ å¥½ï¼æˆ‘æ˜¯ Geminiï¼Œå·²ç¶“é€£æ¥äº†æ‚¨çš„å­—æ¯è¨ˆæ•¸å™¨å·¥å…·ã€‚æ‚¨å¯ä»¥é–‹å§‹æå•äº†ã€‚ï¼ˆè¼¸å…¥ 'exit' çµæŸï¼‰\n",
            "æ‚¨: è«‹å•ä¸‹åˆ—å­—ä¸²ä¸­åˆå…ˆéå¹¾æ¬¡y :  jjkashkdhsjkafhkahfksjyyfasyasyyasdy\n",
            "--- Gemini æ±ºå®šå‘¼å«å·¥å…·: call_letter_counter_tool ---\n",
            "--- æ­£åœ¨å‘¼å«é ç«¯ MCP ä¼ºæœå™¨ ---\n",
            "    ä¼ºæœå™¨: https://cwadayi-mcp-1.hf.space/\n",
            "    åƒæ•¸: word='jjkashkdhsjkafhkahfksjyyfasyasyyasdy', letter='y'\n",
            "Loaded as API: https://cwadayi-mcp-1.hf.space/ âœ”\n",
            "--- MCP ä¼ºæœå™¨å›å‚³çµæœ: 6 ---\n",
            "--- å°‡å·¥å…·çµæœå›å‚³çµ¦ Geminiï¼Œè®“å®ƒç”¢ç”Ÿæœ€çµ‚å›è¦† ---\n",
            "Gemini: æœ‰6å€‹yã€‚\n",
            "\n",
            "æ‚¨: exit\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from gradio_client import Client\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# --- 1. åˆå§‹åŒ–èˆ‡è¨­å®š ---\n",
        "load_dotenv()\n",
        "genai.configure(api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "MCP_SERVER_URL = \"https://cwadayi-mcp-1.hf.space/\"\n",
        "\n",
        "# --- 2. ã€Œè½‰æ¥å™¨ã€å‡½å¼ ---\n",
        "def call_mcp_letter_counter(word: str, letter: str) -> int:\n",
        "    \"\"\"\n",
        "    é€£æ¥åˆ°é ç«¯çš„ Gradio MCP ä¼ºæœå™¨ä¸¦åŸ·è¡Œ letter_counter å·¥å…·ã€‚\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"--- æ­£åœ¨å‘¼å«é ç«¯ MCP ä¼ºæœå™¨ ---\")\n",
        "        print(f\"    ä¼ºæœå™¨: {MCP_SERVER_URL}\")\n",
        "        print(f\"    åƒæ•¸: word='{word}', letter='{letter}'\")\n",
        "\n",
        "        client = Client(src=MCP_SERVER_URL)\n",
        "\n",
        "        #    <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "        #    <<<<< å”¯ä¸€çš„ä¿®æ”¹åœ¨é€™è£¡ >>>>\n",
        "        #    <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "        # ç§»é™¤ api_nameï¼Œè®“ client è‡ªå‹•å‘¼å«é è¨­çš„ç«¯é»\n",
        "        result = client.predict(word, letter)\n",
        "\n",
        "        print(f\"--- MCP ä¼ºæœå™¨å›å‚³çµæœ: {result} ---\")\n",
        "        return int(result)\n",
        "    except Exception as e:\n",
        "        # å°‡æ›´è©³ç´°çš„éŒ¯èª¤è¨Šæ¯å°å‡ºï¼Œæ–¹ä¾¿é™¤éŒ¯\n",
        "        print(f\"å‘¼å« MCP ä¼ºæœå™¨å¤±æ•—: {e}\")\n",
        "        return -1\n",
        "\n",
        "# --- 3. å‘ Gemini å®šç¾©å¯ç”¨çš„å·¥å…· ---\n",
        "letter_counter_tool_declaration = {\n",
        "    \"name\": \"call_letter_counter_tool\",\n",
        "    \"description\": \"è¨ˆç®—ä¸€å€‹æŒ‡å®šçš„å­—æ¯åœ¨ä¸€æ®µæ–‡å­—ä¸­å‡ºç¾äº†å¹¾æ¬¡ã€‚\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"OBJECT\",\n",
        "        \"properties\": {\n",
        "            \"word\": { \"type\": \"STRING\", \"description\": \"è¦åœ¨å…¶ä¸­æœå°‹çš„å®Œæ•´æ–‡å­—ã€‚\" },\n",
        "            \"letter\": { \"type\": \"STRING\", \"description\": \"è¦è¨ˆæ•¸çš„å–®ä¸€å­—å…ƒã€‚\" }\n",
        "        },\n",
        "        \"required\": [\"word\", \"letter\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "# --- 4. å»ºç«‹ Gemini æ¨¡å‹ ---\n",
        "model = genai.GenerativeModel(\n",
        "    model_name='gemini-1.5-flash-latest',\n",
        "    tools=[letter_counter_tool_declaration]\n",
        ")\n",
        "\n",
        "# --- 5. ä¸»åŸ·è¡Œè¿´åœˆ ---\n",
        "print(\"ä½ å¥½ï¼æˆ‘æ˜¯ Geminiï¼Œå·²ç¶“é€£æ¥äº†æ‚¨çš„å­—æ¯è¨ˆæ•¸å™¨å·¥å…·ã€‚æ‚¨å¯ä»¥é–‹å§‹æå•äº†ã€‚ï¼ˆè¼¸å…¥ 'exit' çµæŸï¼‰\")\n",
        "chat = model.start_chat(enable_automatic_function_calling=False)\n",
        "\n",
        "while True:\n",
        "    prompt = input(\"æ‚¨: \")\n",
        "    if prompt.lower() == 'exit':\n",
        "        break\n",
        "\n",
        "    response = chat.send_message(prompt)\n",
        "\n",
        "    function_call = response.candidates[0].content.parts[0].function_call\n",
        "    if function_call:\n",
        "        print(f\"--- Gemini æ±ºå®šå‘¼å«å·¥å…·: {function_call.name} ---\")\n",
        "\n",
        "        if function_call.name == \"call_letter_counter_tool\":\n",
        "            args = function_call.args\n",
        "            tool_result = call_mcp_letter_counter(word=args['word'], letter=args['letter'])\n",
        "\n",
        "            print(\"--- å°‡å·¥å…·çµæœå›å‚³çµ¦ Geminiï¼Œè®“å®ƒç”¢ç”Ÿæœ€çµ‚å›è¦† ---\")\n",
        "            response = chat.send_message(\n",
        "                genai.protos.Part(\n",
        "                    function_response={\n",
        "                        \"name\": \"call_letter_counter_tool\",\n",
        "                        \"response\": {\"result\": tool_result},\n",
        "                    }\n",
        "                )\n",
        "            )\n",
        "            print(f\"Gemini: {response.text}\")\n",
        "    else:\n",
        "        print(f\"Gemini: {response.text}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import google.generativeai as genai\n",
        "from gradio_client import Client\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import userdata # å‡è¨­åœ¨ Colab ç’°å¢ƒ\n",
        "from datetime import datetime\n",
        "\n",
        "# --- 1. åˆå§‹åŒ–èˆ‡è¨­å®š ---\n",
        "# è¼‰å…¥ç’°å¢ƒè®Šæ•¸ (å¦‚æœæ‚¨çš„ API key å„²å­˜åœ¨ .env æª”æ¡ˆä¸­)\n",
        "# load_dotenv()\n",
        "# genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))\n",
        "\n",
        "# ç›´æ¥å¾ Colab secrets è®€å–\n",
        "genai.configure(api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "\n",
        "# æ›´æ–°ç‚ºæ–°çš„ MCP-2 ä¼ºæœå™¨ä½å€\n",
        "# MCP_SERVER_URL = \"cwadayi/MCP-2\"\n",
        "MCP_SERVER_URL = \"https://cwadayi-mcp-2.hf.space\"\n",
        "# --- 2. æ–°çš„ã€Œè½‰æ¥å™¨ã€å‡½å¼ (ç”¨æ–¼åœ°éœ‡æŸ¥è©¢) ---\n",
        "def call_mcp_earthquake_search(\n",
        "    start_date: str,\n",
        "    end_date: str,\n",
        "    min_magnitude: float = 4.5,\n",
        "    max_magnitude: float = 8.0,\n",
        "    lat_from: float = 21.0,\n",
        "    lat_to: float = 26.0,\n",
        "    lon_from: float = 119.0,\n",
        "    lon_to: float = 123.0,\n",
        "    depth_from: float = 0.0,\n",
        "    depth_to: float = 100.0,\n",
        "    start_time: str = \"00:00:00\",\n",
        "    end_time: str = \"23:59:59\"\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    é€£æ¥åˆ°é ç«¯çš„ Gradio MCP ä¼ºæœå™¨ä¸¦åŸ·è¡Œåœ°éœ‡è³‡æ–™æŸ¥è©¢ã€‚\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"--- æ­£åœ¨å‘¼å«é ç«¯åœ°éœ‡ MCP ä¼ºæœå™¨ ---\")\n",
        "        print(f\"    ä¼ºæœå™¨: {MCP_SERVER_URL}\")\n",
        "        print(f\"    æŸ¥è©¢æ¢ä»¶: {start_date} åˆ° {end_date}, è¦æ¨¡ {min_magnitude} ä»¥ä¸Š\")\n",
        "\n",
        "        client = Client(src=MCP_SERVER_URL)\n",
        "\n",
        "        # æ ¹æ“š MCP-2 çš„ API æ–‡ä»¶ï¼Œå‘¼å« `/gradio_fetch_and_plot_data`\n",
        "        # ä¸¦æŒ‰ç…§é †åºå‚³å…¥ 12 å€‹åƒæ•¸\n",
        "        result = client.predict(\n",
        "            param_0=start_date,      # Start Date\n",
        "            param_1=start_time,      # Start Time\n",
        "            param_2=end_date,        # End Date\n",
        "            param_3=end_time,        # End Time\n",
        "            param_4=lat_from,        # Latitude From\n",
        "            param_5=lat_to,          # To\n",
        "            param_6=lon_from,        # Longitude From\n",
        "            param_7=lon_to,          # To\n",
        "            param_8=depth_from,      # Depth From\n",
        "            param_9=depth_to,        # To\n",
        "            param_10=min_magnitude,  # Magnitude From\n",
        "            param_11=max_magnitude,  # To\n",
        "            api_name=\"/gradio_fetch_and_plot_data\"\n",
        "        )\n",
        "\n",
        "        # API å›å‚³ä¸€å€‹åŒ…å«å…©å€‹å…ƒç´ çš„ tuple: (dataframe_dict, plot_dict)\n",
        "        # æˆ‘å€‘éœ€è¦çš„æ˜¯ç¬¬ä¸€å€‹å…ƒç´ çš„è³‡æ–™éƒ¨åˆ†\n",
        "        dataframe_dict = result[0]\n",
        "        headers = dataframe_dict.get('headers', [])\n",
        "        data = dataframe_dict.get('data', [])\n",
        "\n",
        "        if not data:\n",
        "            print(\"--- MCP ä¼ºæœå™¨å›å‚³ï¼šæœªæ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„åœ°éœ‡ ---\")\n",
        "            return \"æŸ¥è©¢å®Œæˆï¼Œä½†æœªæ‰¾åˆ°ä»»ä½•ç¬¦åˆæ¢ä»¶çš„åœ°éœ‡è³‡æ–™ã€‚\"\n",
        "\n",
        "        # å°‡å›å‚³çš„è³‡æ–™è½‰æ›ç‚ºæ›´æ˜“è®€çš„ JSON æ ¼å¼\n",
        "        formatted_results = [dict(zip(headers, row)) for row in data]\n",
        "        json_result = json.dumps(formatted_results, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"--- MCP ä¼ºæœå™¨æˆåŠŸå›å‚³ {len(data)} ç­†è³‡æ–™ ---\")\n",
        "        return json_result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"å‘¼å« MCP ä¼ºæœå™¨å¤±æ•—: {e}\")\n",
        "        return f\"å·¥å…·åŸ·è¡Œå¤±æ•—ï¼ŒéŒ¯èª¤è¨Šæ¯: {e}\"\n",
        "\n",
        "\n",
        "# --- 3. å‘ Gemini å®šç¾©æ–°çš„åœ°éœ‡æŸ¥è©¢å·¥å…· ---\n",
        "earthquake_search_tool_declaration = {\n",
        "    \"name\": \"call_earthquake_search_tool\",\n",
        "    \"description\": \"æ ¹æ“šæŒ‡å®šçš„æ¢ä»¶ï¼ˆæ™‚é–“ã€åœ°é»ã€è¦æ¨¡ç­‰ï¼‰å¾å°ç£ä¸­å¤®æ°£è±¡ç½²çš„è³‡æ–™åº«ä¸­æœå°‹åœ°éœ‡äº‹ä»¶ã€‚é è¨­æœå°‹å°ç£å‘¨é‚Šåœ°å€ã€‚\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"OBJECT\",\n",
        "        \"properties\": {\n",
        "            \"start_date\": {\n",
        "                \"type\": \"STRING\",\n",
        "                \"description\": \"æœå°‹çš„é–‹å§‹æ—¥æœŸï¼Œæ ¼å¼ç‚º 'YYYY-MM-DD'ã€‚ä¾‹å¦‚ï¼š'2024-01-01'ã€‚\"\n",
        "            },\n",
        "            \"end_date\": {\n",
        "                \"type\": \"STRING\",\n",
        "                \"description\": f\"æœå°‹çš„çµæŸæ—¥æœŸï¼Œæ ¼å¼ç‚º 'YYYY-MM-DD'ã€‚ä¾‹å¦‚ï¼š'2024-07-31'ã€‚é è¨­ç‚ºä»Šå¤©: {datetime.now().strftime('%Y-%m-%d')}ã€‚\"\n",
        "            },\n",
        "            \"min_magnitude\": {\n",
        "                \"type\": \"NUMBER\",\n",
        "                \"description\": \"è¦æœå°‹çš„æœ€å°åœ°éœ‡è¦æ¨¡ã€‚é è¨­ç‚º 4.5ã€‚\"\n",
        "            },\n",
        "            \"max_magnitude\": {\n",
        "                \"type\": \"NUMBER\",\n",
        "                \"description\": \"è¦æœå°‹çš„æœ€å¤§åœ°éœ‡è¦æ¨¡ã€‚é è¨­ç‚º 8.0ã€‚\"\n",
        "            },\n",
        "            \"lat_from\": {\"type\": \"NUMBER\", \"description\": \"ç·¯åº¦ç¯„åœçš„èµ·å§‹å€¼ã€‚é è¨­ç‚º 21.0ã€‚\"},\n",
        "            \"lat_to\": {\"type\": \"NUMBER\", \"description\": \"ç·¯åº¦ç¯„åœçš„çµæŸå€¼ã€‚é è¨­ç‚º 26.0ã€‚\"},\n",
        "            \"lon_from\": {\"type\": \"NUMBER\", \"description\": \"ç¶“åº¦ç¯„åœçš„èµ·å§‹å€¼ã€‚é è¨­ç‚º 119.0ã€‚\"},\n",
        "            \"lon_to\": {\"type\": \"NUMBER\", \"description\": \"ç¶“åº¦ç¯„åœçš„çµæŸå€¼ã€‚é è¨­ç‚º 123.0ã€‚\"},\n",
        "        },\n",
        "        \"required\": [\"start_date\", \"end_date\", \"min_magnitude\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "# --- 4. å»ºç«‹ Gemini æ¨¡å‹ ---\n",
        "# å°‡ tools åƒæ•¸æ›´æ–°ç‚ºæ–°çš„å·¥å…·å®šç¾©\n",
        "model = genai.GenerativeModel(\n",
        "    model_name='gemini-1.5-flash-latest',\n",
        "    tools=[earthquake_search_tool_declaration]\n",
        ")\n",
        "\n",
        "\n",
        "# --- 5. ä¸»åŸ·è¡Œè¿´åœˆ ---\n",
        "print(\"ä½ å¥½ï¼æˆ‘æ˜¯ Geminiï¼Œå·²ç¶“é€£æ¥äº†æ‚¨çš„åœ°éœ‡æŸ¥è©¢å·¥å…· ğŸ—ºï¸ã€‚æ‚¨å¯ä»¥é–‹å§‹æå•äº†ã€‚ï¼ˆè¼¸å…¥ 'exit' çµæŸï¼‰\")\n",
        "chat = model.start_chat(enable_automatic_function_calling=False) # æˆ‘å€‘æ‰‹å‹•è™•ç† Function Call\n",
        "\n",
        "while True:\n",
        "    prompt = input(\"æ‚¨: \")\n",
        "    if prompt.lower() == 'exit':\n",
        "        break\n",
        "\n",
        "    try:\n",
        "        response = chat.send_message(prompt)\n",
        "        part = response.candidates[0].content.parts[0]\n",
        "\n",
        "        if part.function_call:\n",
        "            function_call = part.function_call\n",
        "            print(f\"--- Gemini æ±ºå®šå‘¼å«å·¥å…·: {function_call.name} ---\")\n",
        "\n",
        "            if function_call.name == \"call_earthquake_search_tool\":\n",
        "                args = function_call.args\n",
        "                # ç¢ºä¿ end_date æœ‰é è¨­å€¼\n",
        "                if 'end_date' not in args:\n",
        "                    args['end_date'] = datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "                tool_result = call_mcp_earthquake_search(**args) # ä½¿ç”¨ **args å°‡å­—å…¸è§£åŒ…ç‚ºé—œéµå­—åƒæ•¸\n",
        "\n",
        "                print(\"--- å°‡å·¥å…·çµæœå›å‚³çµ¦ Geminiï¼Œè®“å®ƒç”¢ç”Ÿæœ€çµ‚å›è¦† ---\")\n",
        "                response = chat.send_message(\n",
        "                    genai.protos.Part(\n",
        "                        function_response={\n",
        "                            \"name\": \"call_earthquake_search_tool\",\n",
        "                            \"response\": {\"result\": tool_result},\n",
        "                        }\n",
        "                    )\n",
        "                )\n",
        "                print(f\"Gemini: {response.text}\")\n",
        "        else:\n",
        "            print(f\"Gemini: {response.text}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ç™¼ç”ŸéŒ¯èª¤ï¼š{e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "Op98oGe76s74",
        "outputId": "6cee067e-e6e9-4bf4-ea1a-f15e68117a0d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ä½ å¥½ï¼æˆ‘æ˜¯ Geminiï¼Œå·²ç¶“é€£æ¥äº†æ‚¨çš„åœ°éœ‡æŸ¥è©¢å·¥å…· ğŸ—ºï¸ã€‚æ‚¨å¯ä»¥é–‹å§‹æå•äº†ã€‚ï¼ˆè¼¸å…¥ 'exit' çµæŸï¼‰\n",
            "æ‚¨: what is the largest earthquake in 2024 ? show lon,lat,mag,dep,time\n",
            "--- Gemini æ±ºå®šå‘¼å«å·¥å…·: call_earthquake_search_tool ---\n",
            "--- æ­£åœ¨å‘¼å«é ç«¯åœ°éœ‡ MCP ä¼ºæœå™¨ ---\n",
            "    ä¼ºæœå™¨: https://cwadayi-mcp-2.hf.space\n",
            "    æŸ¥è©¢æ¢ä»¶: 2024-01-01 åˆ° 2024-12-31, è¦æ¨¡ 0.0 ä»¥ä¸Š\n",
            "Loaded as API: https://cwadayi-mcp-2.hf.space/ âœ”\n",
            "--- MCP ä¼ºæœå™¨æˆåŠŸå›å‚³ 1000 ç­†è³‡æ–™ ---\n",
            "--- å°‡å·¥å…·çµæœå›å‚³çµ¦ Geminiï¼Œè®“å®ƒç”¢ç”Ÿæœ€çµ‚å›è¦† ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-1.5-flash-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 46951.54ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-1.5-flash-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 5569.23ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-1.5-flash-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 5014.81ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-1.5-flash-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1503.26ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ç™¼ç”ŸéŒ¯èª¤ï¼š429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1348.84ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "æ‚¨: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import google.generativeai as genai\n",
        "from gradio_client import Client\n",
        "from dotenv import load_dotenv\n",
        "from datetime import datetime\n",
        "\n",
        "# --- Colab ä½¿ç”¨è€…è«‹æ³¨æ„ ---\n",
        "# å¦‚æœæ‚¨åœ¨ Google Colab ä¸­åŸ·è¡Œï¼Œè«‹ä½¿ç”¨ä»¥ä¸‹ç¨‹å¼ç¢¼ä¾†è¼‰å…¥æ‚¨çš„ API Keyã€‚\n",
        "# åœ¨å·¦å´é¢æ¿é»æ“Šé‘°åŒ™åœ–ç¤º (Secrets)ï¼Œæ–°å¢ä¸€å€‹åç‚º 'GOOGLE_API_KEY' çš„ secretï¼Œ\n",
        "# ä¸¦å°‡æ‚¨çš„ API Key è²¼å…¥ã€‚\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "# --- æœ¬åœ°ç«¯ä½¿ç”¨è€…è«‹æ³¨æ„ ---\n",
        "# å¦‚æœæ‚¨åœ¨æœ¬åœ°ç«¯åŸ·è¡Œï¼Œè«‹ç¢ºä¿æ‚¨çš„ .env æª”æ¡ˆä¸­æœ‰ 'GOOGLE_API_KEY=\"æ‚¨çš„é‡‘é‘°\"'\n",
        "except (ImportError, ModuleNotFoundError):\n",
        "    load_dotenv()\n",
        "    GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
        "\n",
        "# --- 1. åˆå§‹åŒ–èˆ‡è¨­å®š ---\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# åœ°éœ‡è³‡æ–™æŸ¥è©¢ MCP ä¼ºæœå™¨ä½å€\n",
        "MCP_SERVER_URL = \"https://cwadayi-mcp-2.hf.space\"\n",
        "\n",
        "\n",
        "# --- 2. ã€Œè½‰æ¥å™¨ã€å‡½å¼ (ç”¨æ–¼åœ°éœ‡æŸ¥è©¢) ---\n",
        "def call_mcp_earthquake_search(\n",
        "    start_date: str,\n",
        "    end_date: str,\n",
        "    min_magnitude: float = 4.0,\n",
        "    max_magnitude: float = 9.0,\n",
        "    lat_from: float = 21.0,\n",
        "    lat_to: float = 26.0,\n",
        "    lon_from: float = 119.0,\n",
        "    lon_to: float = 123.0,\n",
        "    depth_from: float = 0.0,\n",
        "    depth_to: float = 100.0,\n",
        "    start_time: str = \"00:00:00\",\n",
        "    end_time: str = \"23:59:59\"\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    é€£æ¥åˆ°é ç«¯çš„ Gradio MCP ä¼ºæœå™¨ä¸¦åŸ·è¡Œåœ°éœ‡è³‡æ–™æŸ¥è©¢ã€‚\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"--- æ­£åœ¨å‘¼å«é ç«¯åœ°éœ‡ MCP ä¼ºæœå™¨ ---\")\n",
        "        print(f\"    ä¼ºæœå™¨: {MCP_SERVER_URL}\")\n",
        "        print(f\"    æŸ¥è©¢æ¢ä»¶: {start_date} åˆ° {end_date}, è¦æ¨¡ {min_magnitude} ä»¥ä¸Š\")\n",
        "\n",
        "        client = Client(src=MCP_SERVER_URL)\n",
        "\n",
        "        # æ ¹æ“š MCP-2 çš„ API æ–‡ä»¶ï¼Œå‘¼å« `/gradio_fetch_and_plot_data`\n",
        "        # ä¸¦æŒ‰ç…§é †åºå‚³å…¥ 12 å€‹åƒæ•¸\n",
        "        result = client.predict(\n",
        "            param_0=start_date,      # Start Date\n",
        "            param_1=start_time,      # Start Time\n",
        "            param_2=end_date,        # End Date\n",
        "            param_3=end_time,        # End Time\n",
        "            param_4=lat_from,        # Latitude From\n",
        "            param_5=lat_to,          # To\n",
        "            param_6=lon_from,        # Longitude From\n",
        "            param_7=lon_to,          # To\n",
        "            param_8=depth_from,      # Depth From\n",
        "            param_9=depth_to,        # To\n",
        "            param_10=min_magnitude,  # Magnitude From\n",
        "            param_11=max_magnitude,  # To\n",
        "            api_name=\"/gradio_fetch_and_plot_data\"\n",
        "        )\n",
        "\n",
        "        # API å›å‚³ä¸€å€‹åŒ…å«å…©å€‹å…ƒç´ çš„ tuple: (dataframe_dict, plot_dict)\n",
        "        # æˆ‘å€‘éœ€è¦çš„æ˜¯ç¬¬ä¸€å€‹å…ƒç´ çš„è³‡æ–™éƒ¨åˆ†\n",
        "        dataframe_dict = result[0]\n",
        "        headers = dataframe_dict.get('headers', [])\n",
        "        data = dataframe_dict.get('data', [])\n",
        "\n",
        "        if not data:\n",
        "            print(\"--- MCP ä¼ºæœå™¨å›å‚³ï¼šæœªæ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„åœ°éœ‡ ---\")\n",
        "            return \"æŸ¥è©¢å®Œæˆï¼Œä½†æœªæ‰¾åˆ°ä»»ä½•ç¬¦åˆæ¢ä»¶çš„åœ°éœ‡è³‡æ–™ã€‚\"\n",
        "\n",
        "        # å°‡å›å‚³çš„è³‡æ–™è½‰æ›ç‚ºæ›´æ˜“è®€çš„ JSON æ ¼å¼\n",
        "        formatted_results = [dict(zip(headers, row)) for row in data]\n",
        "        json_result = json.dumps(formatted_results, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"--- MCP ä¼ºæœå™¨æˆåŠŸå›å‚³ {len(data)} ç­†è³‡æ–™ ---\")\n",
        "        return json_result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"å‘¼å« MCP ä¼ºæœå™¨å¤±æ•—: {e}\")\n",
        "        return f\"å·¥å…·åŸ·è¡Œå¤±æ•—ï¼ŒéŒ¯èª¤è¨Šæ¯: {e}\"\n",
        "\n",
        "\n",
        "# --- 3. å‘ Gemini å®šç¾©æ–°çš„åœ°éœ‡æŸ¥è©¢å·¥å…· ---\n",
        "earthquake_search_tool_declaration = {\n",
        "    \"name\": \"call_earthquake_search_tool\",\n",
        "    \"description\": \"æ ¹æ“šæŒ‡å®šçš„æ¢ä»¶ï¼ˆæ™‚é–“ã€åœ°é»ã€è¦æ¨¡ç­‰ï¼‰å¾å°ç£ä¸­å¤®æ°£è±¡ç½²çš„è³‡æ–™åº«ä¸­æœå°‹åœ°éœ‡äº‹ä»¶ã€‚é è¨­æœå°‹å°ç£å‘¨é‚Šåœ°å€ã€‚\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"OBJECT\",\n",
        "        \"properties\": {\n",
        "            \"start_date\": {\n",
        "                \"type\": \"STRING\",\n",
        "                \"description\": \"æœå°‹çš„é–‹å§‹æ—¥æœŸï¼Œæ ¼å¼ç‚º 'YYYY-MM-DD'ã€‚ä¾‹å¦‚ï¼š'2025-01-01'ã€‚\"\n",
        "            },\n",
        "            \"end_date\": {\n",
        "                \"type\": \"STRING\",\n",
        "                \"description\": f\"æœå°‹çš„çµæŸæ—¥æœŸï¼Œæ ¼å¼ç‚º 'YYYY-MM-DD'ã€‚ä¾‹å¦‚ï¼š'2025-08-14'ã€‚é è¨­ç‚ºä»Šå¤©ã€‚\"\n",
        "            },\n",
        "            \"min_magnitude\": {\n",
        "                \"type\": \"NUMBER\",\n",
        "                \"description\": \"è¦æœå°‹çš„æœ€å°åœ°éœ‡è¦æ¨¡ã€‚é è¨­ç‚º 4.0ã€‚\"\n",
        "            },\n",
        "            \"max_magnitude\": {\n",
        "                \"type\": \"NUMBER\",\n",
        "                \"description\": \"è¦æœå°‹çš„æœ€å¤§åœ°éœ‡è¦æ¨¡ã€‚é è¨­ç‚º 9.0ã€‚\"\n",
        "            },\n",
        "            \"lat_from\": {\"type\": \"NUMBER\", \"description\": \"ç·¯åº¦ç¯„åœçš„èµ·å§‹å€¼ã€‚é è¨­ç‚º 21.0ã€‚\"},\n",
        "            \"lat_to\": {\"type\": \"NUMBER\", \"description\": \"ç·¯åº¦ç¯„åœçš„çµæŸå€¼ã€‚é è¨­ç‚º 26.0ã€‚\"},\n",
        "            \"lon_from\": {\"type\": \"NUMBER\", \"description\": \"ç¶“åº¦ç¯„åœçš„èµ·å§‹å€¼ã€‚é è¨­ç‚º 119.0ã€‚\"},\n",
        "            \"lon_to\": {\"type\": \"NUMBER\", \"description\": \"ç¶“åº¦ç¯„åœçš„çµæŸå€¼ã€‚é è¨­ç‚º 123.0ã€‚\"},\n",
        "        },\n",
        "        \"required\": [\"start_date\", \"end_date\"] # è®“ Gemini å¿…é ˆæä¾›æ—¥æœŸ\n",
        "    }\n",
        "}\n",
        "\n",
        "# --- 4. å»ºç«‹ Gemini æ¨¡å‹ ---\n",
        "model = genai.GenerativeModel(\n",
        "    model_name='gemini-1.5-flash-latest',\n",
        "    tools=[earthquake_search_tool_declaration]\n",
        ")\n",
        "\n",
        "\n",
        "# --- 5. ä¸»åŸ·è¡Œè¿´åœˆ (ä¿®æ­£å¾Œç‰ˆæœ¬) ---\n",
        "print(\"ä½ å¥½ï¼æˆ‘æ˜¯ Geminiï¼Œå·²ç¶“é€£æ¥äº†æ‚¨çš„åœ°éœ‡æŸ¥è©¢å·¥å…· ğŸ—ºï¸ã€‚æ‚¨å¯ä»¥é–‹å§‹æå•äº†ã€‚ï¼ˆè¼¸å…¥ 'exit' çµæŸï¼‰\")\n",
        "chat = model.start_chat(enable_automatic_function_calling=False) # æˆ‘å€‘æ‰‹å‹•è™•ç† Function Call\n",
        "\n",
        "while True:\n",
        "    prompt = input(\"æ‚¨: \")\n",
        "    if prompt.lower() == 'exit':\n",
        "        break\n",
        "\n",
        "    try:\n",
        "        response = chat.send_message(prompt)\n",
        "\n",
        "        # --- ç©©å¥çš„å›æ‡‰è™•ç†é‚è¼¯ ---\n",
        "        has_function_call = False\n",
        "        text_response = \"\"\n",
        "\n",
        "        for part in response.candidates[0].content.parts:\n",
        "            if part.function_call:\n",
        "                function_call = part.function_call\n",
        "                print(f\"--- Gemini æ±ºå®šå‘¼å«å·¥å…·: {function_call.name} ---\")\n",
        "\n",
        "                if function_call.name == \"call_earthquake_search_tool\":\n",
        "                    has_function_call = True\n",
        "                    args = function_call.args\n",
        "                    # å¦‚æœä½¿ç”¨è€…æ²’æä¾›çµæŸæ—¥æœŸï¼Œè‡ªå‹•è¨­ç‚ºä»Šå¤©\n",
        "                    if 'end_date' not in args:\n",
        "                        args['end_date'] = datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "                    tool_result = call_mcp_earthquake_search(**args)\n",
        "\n",
        "                    print(\"--- å°‡å·¥å…·çµæœå›å‚³çµ¦ Geminiï¼Œè®“å®ƒç”¢ç”Ÿæœ€çµ‚å›è¦† ---\")\n",
        "                    # å°‡å·¥å…·çš„åŸ·è¡Œçµæœå›å‚³çµ¦æ¨¡å‹\n",
        "                    response = chat.send_message(\n",
        "                        genai.protos.Part(\n",
        "                            function_response={\n",
        "                                \"name\": \"call_earthquake_search_tool\",\n",
        "                                \"response\": {\"result\": tool_result},\n",
        "                            }\n",
        "                        )\n",
        "                    )\n",
        "                    # è™•ç†å·¥å…·åŸ·è¡Œå¾Œçš„æœ€çµ‚å›è¦†\n",
        "                    for final_part in response.candidates[0].content.parts:\n",
        "                        text_response += final_part.text\n",
        "\n",
        "            elif part.text:\n",
        "                 text_response += part.text\n",
        "\n",
        "        # è¿´åœˆçµæŸå¾Œï¼Œå°å‡ºæ”¶é›†åˆ°çš„æ‰€æœ‰æ–‡å­—å›æ‡‰\n",
        "        if text_response:\n",
        "             print(f\"Gemini: {text_response}\")\n",
        "        # å¦‚æœæ¨¡å‹æ²’æœ‰ç”¢ç”Ÿä»»ä½•æ–‡å­—ï¼Œä¹Ÿæ²’æœ‰å‘¼å«å‡½å¼ï¼Œçµ¦ä¸€å€‹é€šç”¨å›è¦†\n",
        "        elif not has_function_call:\n",
        "             print(\"Gemini: æˆ‘ä¸ç¢ºå®šè©²å¦‚ä½•å›æ‡‰ã€‚\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ç™¼ç”ŸéŒ¯èª¤ï¼š{e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "ngEThueh_wVh",
        "outputId": "0bccd337-3e21-4ecb-9840-8a5d501ec8c7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ä½ å¥½ï¼æˆ‘æ˜¯ Geminiï¼Œå·²ç¶“é€£æ¥äº†æ‚¨çš„åœ°éœ‡æŸ¥è©¢å·¥å…· ğŸ—ºï¸ã€‚æ‚¨å¯ä»¥é–‹å§‹æå•äº†ã€‚ï¼ˆè¼¸å…¥ 'exit' çµæŸï¼‰\n",
            "æ‚¨: what is the largest earthquake in 2024 ?\n",
            "Gemini: I need to know the available data to answer your question.  The available tools don't specify how to get the largest earthquake within a year.  Can you provide more information or different tools?\n",
            "\n",
            "æ‚¨: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O_uaRbQb_vte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "id": "hbwK1PfKM1gw",
        "outputId": "b98ef17d-0f39-42be-b63c-0e51717c2d21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "æ­£åœ¨æŸ¥è©¢æ‚¨å¸³è™Ÿå¯ç”¨çš„æ¨¡å‹...\n",
            "\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemini-1.5-pro-latest\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemini-1.5-pro-002\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemini-1.5-pro\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemini-1.5-flash-latest\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemini-1.5-flash\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemini-1.5-flash-002\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemini-1.5-flash-8b\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemini-1.5-flash-8b-001\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemini-1.5-flash-8b-latest\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemini-2.5-pro-preview-03-25\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemini-2.5-flash-preview-05-20\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemini-2.5-flash\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemini-2.5-flash-lite-preview-06-17\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemini-2.5-pro-preview-05-06\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemini-2.5-pro-preview-06-05\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemini-2.5-pro\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemini-2.0-flash-exp\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemini-2.0-flash\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemini-2.0-flash-001\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemini-2.0-flash-exp-image-generation\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemini-2.0-flash-lite-001\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemini-2.0-flash-lite\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemini-2.0-flash-preview-image-generation\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemini-2.0-flash-lite-preview-02-05\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemini-2.0-flash-lite-preview\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemini-2.0-pro-exp\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemini-2.0-pro-exp-02-05\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemini-exp-1206\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemini-2.0-flash-thinking-exp-01-21\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemini-2.0-flash-thinking-exp\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemini-2.0-flash-thinking-exp-1219\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemini-2.5-flash-preview-tts\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemini-2.5-pro-preview-tts\n",
            "-> å¯ç”¨æ¨¡å‹: models/learnlm-2.0-flash-experimental\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemma-3-1b-it\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemma-3-4b-it\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemma-3-12b-it\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemma-3-27b-it\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemma-3n-e4b-it\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemma-3n-e2b-it\n",
            "-> å¯ç”¨æ¨¡å‹: models/gemini-2.5-flash-lite\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# è¼‰å…¥æ‚¨çš„ API é‡‘é‘°\n",
        "load_dotenv()\n",
        "genai.configure(api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "\n",
        "print(\"æ­£åœ¨æŸ¥è©¢æ‚¨å¸³è™Ÿå¯ç”¨çš„æ¨¡å‹...\\n\")\n",
        "\n",
        "# éæ­·æ‰€æœ‰æ¨¡å‹\n",
        "for model in genai.list_models():\n",
        "  # æª¢æŸ¥è©²æ¨¡å‹æ˜¯å¦æ”¯æ´ 'generateContent' æ–¹æ³•ï¼ˆèŠå¤©å’Œç”Ÿæˆå…§å®¹æ‰€å¿…éœ€çš„ï¼‰\n",
        "  if 'generateContent' in model.supported_generation_methods:\n",
        "    print(f\"-> å¯ç”¨æ¨¡å‹: {model.name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "8NgC6X5wPZ1t",
        "outputId": "6d071b0c-cb19-47f0-c5ca-5d37bb8e7dc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ä½ å¥½ï¼æˆ‘æ˜¯ Geminiï¼Œå·²ç¶“é€£æ¥äº†æ‚¨çš„å­—æ¯è¨ˆæ•¸å™¨å·¥å…·ã€‚æ‚¨å¯ä»¥é–‹å§‹æå•äº†ã€‚ï¼ˆè¼¸å…¥ 'exit' çµæŸï¼‰\n",
            "æ‚¨: 2015å¹´ä»¥å¾Œè¦æ¨¡å¤§æ–¼5.0çš„åœ°éœ‡æœ‰å¹¾å€‹\n",
            "Gemini: I do not have access to real-time information, including earthquake data.  Therefore, I cannot answer how many earthquakes larger than magnitude 5.0 occurred after 2015.  To find this information, you would need to consult a seismic database such as those maintained by the USGS (United States Geological Survey) or other similar organizations.\n",
            "\n",
            "æ‚¨: exit\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from gradio_client import Client\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# --- 1. åˆå§‹åŒ–èˆ‡è¨­å®š ---\n",
        "load_dotenv()\n",
        "genai.configure(api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "MCP_SERVER_URL = \"https://cwadayi-mcp-2.hf.space/\"\n",
        "\n",
        "# --- 2. ã€Œè½‰æ¥å™¨ã€å‡½å¼ ---\n",
        "def call_mcp_letter_counter(word: str, letter: str) -> int:\n",
        "    \"\"\"\n",
        "    é€£æ¥åˆ°é ç«¯çš„ Gradio MCP ä¼ºæœå™¨ä¸¦åŸ·è¡Œ letter_counter å·¥å…·ã€‚\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"--- æ­£åœ¨å‘¼å«é ç«¯ MCP ä¼ºæœå™¨ ---\")\n",
        "        print(f\"    ä¼ºæœå™¨: {MCP_SERVER_URL}\")\n",
        "        print(f\"    åƒæ•¸: word='{word}', letter='{letter}'\")\n",
        "\n",
        "        client = Client(src=MCP_SERVER_URL)\n",
        "\n",
        "        #    <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "        #    <<<<< å”¯ä¸€çš„ä¿®æ”¹åœ¨é€™è£¡ >>>>\n",
        "        #    <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "        # ç§»é™¤ api_nameï¼Œè®“ client è‡ªå‹•å‘¼å«é è¨­çš„ç«¯é»\n",
        "        result = client.predict(word, letter)\n",
        "\n",
        "        print(f\"--- MCP ä¼ºæœå™¨å›å‚³çµæœ: {result} ---\")\n",
        "        return int(result)\n",
        "    except Exception as e:\n",
        "        # å°‡æ›´è©³ç´°çš„éŒ¯èª¤è¨Šæ¯å°å‡ºï¼Œæ–¹ä¾¿é™¤éŒ¯\n",
        "        print(f\"å‘¼å« MCP ä¼ºæœå™¨å¤±æ•—: {e}\")\n",
        "        return -1\n",
        "\n",
        "# --- 3. å‘ Gemini å®šç¾©å¯ç”¨çš„å·¥å…· ---\n",
        "letter_counter_tool_declaration = {\n",
        "    \"name\": \"call_letter_counter_tool\",\n",
        "    \"description\": \"è¨ˆç®—ä¸€å€‹æŒ‡å®šçš„å­—æ¯åœ¨ä¸€æ®µæ–‡å­—ä¸­å‡ºç¾äº†å¹¾æ¬¡ã€‚\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"OBJECT\",\n",
        "        \"properties\": {\n",
        "            \"word\": { \"type\": \"STRING\", \"description\": \"è¦åœ¨å…¶ä¸­æœå°‹çš„å®Œæ•´æ–‡å­—ã€‚\" },\n",
        "            \"letter\": { \"type\": \"STRING\", \"description\": \"è¦è¨ˆæ•¸çš„å–®ä¸€å­—å…ƒã€‚\" }\n",
        "        },\n",
        "        \"required\": [\"word\", \"letter\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "# --- 4. å»ºç«‹ Gemini æ¨¡å‹ ---\n",
        "model = genai.GenerativeModel(\n",
        "    model_name='gemini-1.5-flash-latest',\n",
        "    tools=[letter_counter_tool_declaration]\n",
        ")\n",
        "\n",
        "# --- 5. ä¸»åŸ·è¡Œè¿´åœˆ ---\n",
        "print(\"ä½ å¥½ï¼æˆ‘æ˜¯ Geminiï¼Œå·²ç¶“é€£æ¥äº†æ‚¨çš„å­—æ¯è¨ˆæ•¸å™¨å·¥å…·ã€‚æ‚¨å¯ä»¥é–‹å§‹æå•äº†ã€‚ï¼ˆè¼¸å…¥ 'exit' çµæŸï¼‰\")\n",
        "chat = model.start_chat(enable_automatic_function_calling=False)\n",
        "\n",
        "while True:\n",
        "    prompt = input(\"æ‚¨: \")\n",
        "    if prompt.lower() == 'exit':\n",
        "        break\n",
        "\n",
        "    response = chat.send_message(prompt)\n",
        "\n",
        "    function_call = response.candidates[0].content.parts[0].function_call\n",
        "    if function_call:\n",
        "        print(f\"--- Gemini æ±ºå®šå‘¼å«å·¥å…·: {function_call.name} ---\")\n",
        "\n",
        "        if function_call.name == \"call_letter_counter_tool\":\n",
        "            args = function_call.args\n",
        "            tool_result = call_mcp_letter_counter(word=args['word'], letter=args['letter'])\n",
        "\n",
        "            print(\"--- å°‡å·¥å…·çµæœå›å‚³çµ¦ Geminiï¼Œè®“å®ƒç”¢ç”Ÿæœ€çµ‚å›è¦† ---\")\n",
        "            response = chat.send_message(\n",
        "                genai.protos.Part(\n",
        "                    function_response={\n",
        "                        \"name\": \"call_letter_counter_tool\",\n",
        "                        \"response\": {\"result\": tool_result},\n",
        "                    }\n",
        "                )\n",
        "            )\n",
        "            print(f\"Gemini: {response.text}\")\n",
        "    else:\n",
        "        print(f\"Gemini: {response.text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKaaS5THPce5"
      },
      "outputs": [],
      "source": [
        "api_key = userdata.get('GOOGLE_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QfKBNR4NAd-",
        "outputId": "fa9d7593-e157-49c1-88f0-5d0e5eb1e525"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:__main__:Failed to initialize: HTTP 404\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to connect to MCP server\n",
            "\n",
            "============================================================\n",
            "EVENT LOOP ISSUE DETECTED\n",
            "============================================================\n",
            "If you're in Jupyter notebook, use these commands instead:\n",
            "\n",
            "# Set your API key\n",
            "api_key = 'your-gemini-api-key'\n",
            "\n",
            "# Quick analysis\n",
            "analysis = await quick_analysis(api_key)\n",
            "print(analysis)\n",
            "\n",
            "# Ask a question\n",
            "answer = await ask_earthquake_question(api_key, 'What was the strongest earthquake?')\n",
            "print(answer)\n",
            "\n",
            "# Create analyst for multiple queries\n",
            "analyst = await create_analyst(api_key)\n",
            "if analyst:\n",
            "    result = await analyst.analyze_earthquake_data({'ML_min': 5.0})\n",
            "    print(result)\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Google Gemini MCP Client for Earthquake Data Server\n",
        "Connects to the earthquake data MCP server deployed on Hugging Face Spaces\n",
        "and uses Gemini AI to provide intelligent earthquake data analysis.\n",
        "\"\"\"\n",
        "\n",
        "import asyncio\n",
        "import json\n",
        "import logging\n",
        "import aiohttp\n",
        "import google.generativeai as genai\n",
        "from typing import Dict, Any, List, Optional\n",
        "from datetime import datetime\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@dataclass\n",
        "class MCPResponse:\n",
        "    \"\"\"Data class for MCP response\"\"\"\n",
        "    success: bool\n",
        "    data: Optional[Dict[str, Any]] = None\n",
        "    error: Optional[str] = None\n",
        "\n",
        "class EarthquakeMCPClient:\n",
        "    \"\"\"MCP Client for connecting to earthquake data server\"\"\"\n",
        "\n",
        "    def __init__(self, server_url: str):\n",
        "        self.server_url = server_url.rstrip('/')\n",
        "        self.mcp_endpoint = f\"{self.server_url}/mcp\"  # Assuming MCP endpoint\n",
        "        self.session = None\n",
        "        self.request_id = 0\n",
        "\n",
        "    async def __aenter__(self):\n",
        "        \"\"\"Async context manager entry\"\"\"\n",
        "        self.session = aiohttp.ClientSession()\n",
        "        return self\n",
        "\n",
        "    async def __aexit__(self, exc_type, exc_val, exc_tb):\n",
        "        \"\"\"Async context manager exit\"\"\"\n",
        "        if self.session:\n",
        "            await self.session.close()\n",
        "\n",
        "    def _get_next_id(self) -> int:\n",
        "        \"\"\"Get next request ID\"\"\"\n",
        "        self.request_id += 1\n",
        "        return self.request_id\n",
        "\n",
        "    async def _make_mcp_request(self, method: str, params: Dict[str, Any] = None) -> MCPResponse:\n",
        "        \"\"\"Make MCP request to the server\"\"\"\n",
        "        if not self.session:\n",
        "            raise RuntimeError(\"Client not initialized. Use async context manager.\")\n",
        "\n",
        "        request_data = {\n",
        "            \"jsonrpc\": \"2.0\",\n",
        "            \"id\": self._get_next_id(),\n",
        "            \"method\": method,\n",
        "            \"params\": params or {}\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Try direct HTTP request to Gradio MCP interface\n",
        "            async with self.session.post(\n",
        "                f\"{self.server_url}/api/predict\",\n",
        "                json={\n",
        "                    \"data\": [json.dumps(request_data)],\n",
        "                    \"fn_index\": 1  # Assuming MCP handler is at index 1\n",
        "                },\n",
        "                headers={\"Content-Type\": \"application/json\"},\n",
        "                timeout=30\n",
        "            ) as response:\n",
        "                if response.status == 200:\n",
        "                    result = await response.json()\n",
        "                    # Extract response from Gradio format\n",
        "                    if \"data\" in result and result[\"data\"]:\n",
        "                        mcp_response = json.loads(result[\"data\"][0])\n",
        "                        if \"error\" in mcp_response:\n",
        "                            return MCPResponse(success=False, error=mcp_response[\"error\"][\"message\"])\n",
        "                        return MCPResponse(success=True, data=mcp_response.get(\"result\"))\n",
        "                    else:\n",
        "                        return MCPResponse(success=False, error=\"Invalid response format\")\n",
        "                else:\n",
        "                    return MCPResponse(success=False, error=f\"HTTP {response.status}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"MCP request failed: {e}\")\n",
        "            return MCPResponse(success=False, error=str(e))\n",
        "\n",
        "    async def initialize(self) -> MCPResponse:\n",
        "        \"\"\"Initialize MCP connection\"\"\"\n",
        "        return await self._make_mcp_request(\"initialize\", {\n",
        "            \"protocolVersion\": \"2024-11-05\",\n",
        "            \"capabilities\": {},\n",
        "            \"clientInfo\": {\n",
        "                \"name\": \"gemini-mcp-client\",\n",
        "                \"version\": \"1.0.0\"\n",
        "            }\n",
        "        })\n",
        "\n",
        "    async def list_tools(self) -> MCPResponse:\n",
        "        \"\"\"List available tools\"\"\"\n",
        "        return await self._make_mcp_request(\"tools/list\")\n",
        "\n",
        "    async def call_tool(self, tool_name: str, arguments: Dict[str, Any] = None) -> MCPResponse:\n",
        "        \"\"\"Call a specific tool\"\"\"\n",
        "        return await self._make_mcp_request(\"tools/call\", {\n",
        "            \"name\": tool_name,\n",
        "            \"arguments\": arguments or {}\n",
        "        })\n",
        "\n",
        "class GeminiEarthquakeAnalyst:\n",
        "    \"\"\"Gemini AI analyst for earthquake data\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str, model_name: str = \"gemini-pro\"):\n",
        "        genai.configure(api_key=api_key)\n",
        "        self.model = genai.GenerativeModel(model_name)\n",
        "        self.mcp_client = None\n",
        "\n",
        "    async def connect_to_mcp_server(self, server_url: str):\n",
        "        \"\"\"Connect to MCP server\"\"\"\n",
        "        self.mcp_client = EarthquakeMCPClient(server_url)\n",
        "\n",
        "    async def initialize_connection(self) -> bool:\n",
        "        \"\"\"Initialize MCP connection\"\"\"\n",
        "        if not self.mcp_client:\n",
        "            logger.error(\"MCP client not connected\")\n",
        "            return False\n",
        "\n",
        "        async with self.mcp_client as client:\n",
        "            # Initialize connection\n",
        "            init_response = await client.initialize()\n",
        "            if not init_response.success:\n",
        "                logger.error(f\"Failed to initialize: {init_response.error}\")\n",
        "                return False\n",
        "\n",
        "            # List available tools\n",
        "            tools_response = await client.list_tools()\n",
        "            if not tools_response.success:\n",
        "                logger.error(f\"Failed to list tools: {tools_response.error}\")\n",
        "                return False\n",
        "\n",
        "            logger.info(\"Successfully connected to MCP server\")\n",
        "            logger.info(f\"Available tools: {list(tools_response.data.get('tools', []))}\")\n",
        "            return True\n",
        "\n",
        "    async def query_earthquakes(self, **kwargs) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"Query earthquake data\"\"\"\n",
        "        async with self.mcp_client as client:\n",
        "            response = await client.call_tool(\"query_earthquakes\", kwargs)\n",
        "            if response.success:\n",
        "                return json.loads(response.data[\"content\"][0][\"text\"])\n",
        "            else:\n",
        "                logger.error(f\"Query failed: {response.error}\")\n",
        "                return None\n",
        "\n",
        "    async def get_earthquake_stats(self, **kwargs) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"Get earthquake statistics\"\"\"\n",
        "        async with self.mcp_client as client:\n",
        "            response = await client.call_tool(\"get_earthquake_stats\", kwargs)\n",
        "            if response.success:\n",
        "                return json.loads(response.data[\"content\"][0][\"text\"])\n",
        "            else:\n",
        "                logger.error(f\"Stats query failed: {response.error}\")\n",
        "                return None\n",
        "\n",
        "    async def create_earthquake_map(self, **kwargs) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"Create earthquake map\"\"\"\n",
        "        async with self.mcp_client as client:\n",
        "            response = await client.call_tool(\"create_earthquake_map\",\n",
        "                                            {**kwargs, \"return_base64\": True})\n",
        "            if response.success:\n",
        "                return json.loads(response.data[\"content\"][0][\"text\"])\n",
        "            else:\n",
        "                logger.error(f\"Map creation failed: {response.error}\")\n",
        "                return None\n",
        "\n",
        "    def _format_earthquake_data_for_gemini(self, data: Dict[str, Any]) -> str:\n",
        "        \"\"\"Format earthquake data for Gemini analysis\"\"\"\n",
        "        if not data.get(\"success\"):\n",
        "            return f\"Error retrieving data: {data.get('error', 'Unknown error')}\"\n",
        "\n",
        "        if data.get(\"count\", 0) == 0:\n",
        "            return \"No earthquake data found for the specified criteria.\"\n",
        "\n",
        "        formatted = f\"\"\"\n",
        "Earthquake Data Summary:\n",
        "- Total earthquakes: {data.get('count', 0)}\n",
        "- Date range: Query parameters used\n",
        "        \"\"\"\n",
        "\n",
        "        if \"summary\" in data:\n",
        "            summary = data[\"summary\"]\n",
        "            formatted += f\"\"\"\n",
        "- Magnitude range: {summary.get('magnitude_range', [])}\n",
        "- Depth range: {summary.get('depth_range', [])} km\n",
        "- Location bounds: {summary.get('location_bounds', {})}\n",
        "            \"\"\"\n",
        "\n",
        "        if \"data\" in data and data[\"data\"]:\n",
        "            formatted += f\"\\nSample earthquakes:\\n\"\n",
        "            for i, eq in enumerate(data[\"data\"][:5]):  # Show first 5\n",
        "                formatted += f\"- Date: {eq.get('date', 'N/A')}, Magnitude: {eq.get('ML', 'N/A')}, Depth: {eq.get('depth', 'N/A')}km\\n\"\n",
        "\n",
        "        return formatted.strip()\n",
        "\n",
        "    def _format_stats_for_gemini(self, stats: Dict[str, Any]) -> str:\n",
        "        \"\"\"Format statistics for Gemini analysis\"\"\"\n",
        "        if not stats.get(\"success\"):\n",
        "            return f\"Error retrieving statistics: {stats.get('error', 'Unknown error')}\"\n",
        "\n",
        "        formatted = f\"\"\"\n",
        "Earthquake Statistics:\n",
        "- Total earthquakes: {stats.get('total_earthquakes', 0)}\n",
        "- Date range: {stats.get('date_range', {})}\n",
        "\n",
        "Magnitude Statistics:\n",
        "- Range: {stats.get('magnitude_stats', {}).get('min', 'N/A')} - {stats.get('magnitude_stats', {}).get('max', 'N/A')}\n",
        "- Mean: {stats.get('magnitude_stats', {}).get('mean', 'N/A'):.2f}\n",
        "- Median: {stats.get('magnitude_stats', {}).get('median', 'N/A'):.2f}\n",
        "\n",
        "Depth Statistics:\n",
        "- Range: {stats.get('depth_stats', {}).get('min', 'N/A')} - {stats.get('depth_stats', {}).get('max', 'N/A')} km\n",
        "- Mean: {stats.get('depth_stats', {}).get('mean', 'N/A'):.2f} km\n",
        "\n",
        "Magnitude Distribution:\n",
        "{json.dumps(stats.get('magnitude_distribution', {}), indent=2)}\n",
        "\n",
        "Depth Distribution:\n",
        "{json.dumps(stats.get('depth_distribution', {}), indent=2)}\n",
        "        \"\"\"\n",
        "\n",
        "        return formatted.strip()\n",
        "\n",
        "    async def analyze_earthquake_data(self, query_params: Dict[str, Any] = None) -> str:\n",
        "        \"\"\"Analyze earthquake data using Gemini\"\"\"\n",
        "        try:\n",
        "            # Get earthquake data\n",
        "            data = await self.query_earthquakes(**(query_params or {}))\n",
        "            if not data:\n",
        "                return \"Failed to retrieve earthquake data.\"\n",
        "\n",
        "            # Get statistics\n",
        "            stats = await self.get_earthquake_stats(**(query_params or {}))\n",
        "\n",
        "            # Format data for Gemini\n",
        "            data_text = self._format_earthquake_data_for_gemini(data)\n",
        "            stats_text = self._format_stats_for_gemini(stats) if stats else \"\"\n",
        "\n",
        "            # Create prompt for Gemini\n",
        "            prompt = f\"\"\"\n",
        "Please analyze the following earthquake data and provide insights:\n",
        "\n",
        "{data_text}\n",
        "\n",
        "{stats_text}\n",
        "\n",
        "Please provide:\n",
        "1. A summary of the earthquake activity\n",
        "2. Notable patterns or trends\n",
        "3. Risk assessment based on the data\n",
        "4. Recommendations for further analysis\n",
        "5. Any significant observations about magnitude, depth, or location patterns\n",
        "\n",
        "Be specific and reference the actual data in your analysis.\n",
        "            \"\"\"\n",
        "\n",
        "            # Generate analysis using Gemini\n",
        "            response = self.model.generate_content(prompt)\n",
        "            return response.text\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Analysis failed: {e}\")\n",
        "            return f\"Analysis failed: {str(e)}\"\n",
        "\n",
        "    async def answer_earthquake_question(self, question: str,\n",
        "                                       query_params: Dict[str, Any] = None) -> str:\n",
        "        \"\"\"Answer specific questions about earthquake data\"\"\"\n",
        "        try:\n",
        "            # Get relevant data based on the question\n",
        "            data = await self.query_earthquakes(**(query_params or {}))\n",
        "            stats = await self.get_earthquake_stats(**(query_params or {}))\n",
        "\n",
        "            if not data:\n",
        "                return \"I couldn't retrieve earthquake data to answer your question.\"\n",
        "\n",
        "            # Format data for context\n",
        "            data_context = self._format_earthquake_data_for_gemini(data)\n",
        "            stats_context = self._format_stats_for_gemini(stats) if stats else \"\"\n",
        "\n",
        "            prompt = f\"\"\"\n",
        "Based on the following earthquake data, please answer this question: \"{question}\"\n",
        "\n",
        "Earthquake Data:\n",
        "{data_context}\n",
        "\n",
        "Statistics:\n",
        "{stats_context}\n",
        "\n",
        "Please provide a detailed, accurate answer based on the actual data provided. If the data doesn't contain enough information to answer the question, please say so.\n",
        "            \"\"\"\n",
        "\n",
        "            response = self.model.generate_content(prompt)\n",
        "            return response.text\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Question answering failed: {e}\")\n",
        "            return f\"I encountered an error: {str(e)}\"\n",
        "\n",
        "async def main():\n",
        "    \"\"\"Main function demonstrating the MCP client usage\"\"\"\n",
        "\n",
        "    # Configuration\n",
        "    GEMINI_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    MCP_SERVER_URL = \"https://cwadayi-mcp-2.hf.space\"\n",
        "\n",
        "    if not GEMINI_API_KEY:\n",
        "        print(\"Please set GEMINI_API_KEY environment variable\")\n",
        "        return\n",
        "\n",
        "    # Initialize Gemini analyst\n",
        "    analyst = GeminiEarthquakeAnalyst(GEMINI_API_KEY)\n",
        "    await analyst.connect_to_mcp_server(MCP_SERVER_URL)\n",
        "\n",
        "    # Test connection\n",
        "    if not await analyst.initialize_connection():\n",
        "        print(\"Failed to connect to MCP server\")\n",
        "        return\n",
        "\n",
        "    print(\"=== Gemini Earthquake Data Analyst ===\\n\")\n",
        "\n",
        "    # Example 1: General analysis\n",
        "    print(\"1. General earthquake analysis for Taiwan region (2024):\")\n",
        "    analysis = await analyst.analyze_earthquake_data({\n",
        "        \"start_date\": \"2024-01-01\",\n",
        "        \"end_date\": \"2024-12-31\",\n",
        "        \"ML_min\": 4.0\n",
        "    })\n",
        "    print(analysis)\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    # Example 2: Answer specific question\n",
        "    print(\"2. Answering specific question:\")\n",
        "    question = \"What was the strongest earthquake in the first quarter of 2024?\"\n",
        "    answer = await analyst.answer_earthquake_question(\n",
        "        question,\n",
        "        {\n",
        "            \"start_date\": \"2024-01-01\",\n",
        "            \"end_date\": \"2024-03-31\",\n",
        "            \"ML_min\": 4.0\n",
        "        }\n",
        "    )\n",
        "    print(f\"Q: {question}\")\n",
        "    print(f\"A: {answer}\")\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    return analyst  # Return analyst for interactive use\n",
        "\n",
        "def run_main():\n",
        "    \"\"\"Run main function with proper event loop handling\"\"\"\n",
        "    try:\n",
        "        # Check if we're in a running event loop (like Jupyter)\n",
        "        loop = asyncio.get_running_loop()\n",
        "        if loop and loop.is_running():\n",
        "            # We're in a running event loop, create a task instead\n",
        "            import nest_asyncio\n",
        "            nest_asyncio.apply()\n",
        "            return asyncio.run(main())\n",
        "        else:\n",
        "            return asyncio.run(main())\n",
        "    except RuntimeError:\n",
        "        # Fallback: try to run in existing loop\n",
        "        try:\n",
        "            import nest_asyncio\n",
        "            nest_asyncio.apply()\n",
        "            return asyncio.run(main())\n",
        "        except ImportError:\n",
        "            print(\"For Jupyter notebook, please install nest_asyncio: pip install nest_asyncio\")\n",
        "            print(\"Or use the notebook-friendly functions below:\")\n",
        "            return None\n",
        "\n",
        "# Notebook-friendly wrapper functions\n",
        "async def quick_analysis(api_key: str,\n",
        "                        start_date: str = \"2024-01-01\",\n",
        "                        end_date: str = \"2024-12-31\",\n",
        "                        ml_min: float = 4.0):\n",
        "    \"\"\"Quick analysis function for notebook use\"\"\"\n",
        "    analyst = GeminiEarthquakeAnalyst(api_key)\n",
        "    await analyst.connect_to_mcp_server(\"https://cwadayi-mcp-2.hf.space\")\n",
        "\n",
        "    if await analyst.initialize_connection():\n",
        "        analysis = await analyst.analyze_earthquake_data({\n",
        "            \"start_date\": start_date,\n",
        "            \"end_date\": end_date,\n",
        "            \"ML_min\": ml_min\n",
        "        })\n",
        "        return analysis\n",
        "    return \"Failed to connect to MCP server\"\n",
        "\n",
        "async def ask_earthquake_question(api_key: str,\n",
        "                                question: str,\n",
        "                                start_date: str = \"2024-01-01\",\n",
        "                                end_date: str = \"2024-12-31\",\n",
        "                                ml_min: float = 4.0):\n",
        "    \"\"\"Ask a question about earthquakes\"\"\"\n",
        "    analyst = GeminiEarthquakeAnalyst(api_key)\n",
        "    await analyst.connect_to_mcp_server(\"https://cwadayi-mcp-2.hf.space\")\n",
        "\n",
        "    if await analyst.initialize_connection():\n",
        "        answer = await analyst.answer_earthquake_question(\n",
        "            question,\n",
        "            {\n",
        "                \"start_date\": start_date,\n",
        "                \"end_date\": end_date,\n",
        "                \"ML_min\": ml_min\n",
        "            }\n",
        "        )\n",
        "        return answer\n",
        "    return \"Failed to connect to MCP server\"\n",
        "\n",
        "async def create_analyst(api_key: str):\n",
        "    \"\"\"Create and initialize analyst for interactive use\"\"\"\n",
        "    analyst = GeminiEarthquakeAnalyst(api_key)\n",
        "    await analyst.connect_to_mcp_server(\"https://cwadayi-mcp-2.hf.space\")\n",
        "\n",
        "    if await analyst.initialize_connection():\n",
        "        return analyst\n",
        "    return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Try to run main, handle event loop issues\n",
        "    analyst = run_main()\n",
        "\n",
        "    # If main didn't run due to event loop issues, provide alternative\n",
        "    if analyst is None:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"EVENT LOOP ISSUE DETECTED\")\n",
        "        print(\"=\"*60)\n",
        "        print(\"If you're in Jupyter notebook, use these commands instead:\")\n",
        "        print()\n",
        "        print(\"# Set your API key\")\n",
        "        print(\"api_key = 'your-gemini-api-key'\")\n",
        "        print()\n",
        "        print(\"# Quick analysis\")\n",
        "        print(\"analysis = await quick_analysis(api_key)\")\n",
        "        print(\"print(analysis)\")\n",
        "        print()\n",
        "        print(\"# Ask a question\")\n",
        "        print(\"answer = await ask_earthquake_question(api_key, 'What was the strongest earthquake?')\")\n",
        "        print(\"print(answer)\")\n",
        "        print()\n",
        "        print(\"# Create analyst for multiple queries\")\n",
        "        print(\"analyst = await create_analyst(api_key)\")\n",
        "        print(\"if analyst:\")\n",
        "        print(\"    result = await analyst.analyze_earthquake_data({'ML_min': 5.0})\")\n",
        "        print(\"    print(result)\")\n",
        "        print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsPHSZLiWToTCuPaqnL4ul",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}