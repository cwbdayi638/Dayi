{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cwbdayi638/Dayi/blob/main/MCP_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Prug3cAQL08E"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uwv5-PeDL7Pc",
        "outputId": "d0948dd6-972e-4293-b892-42a425e0f8e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: gradio_client in /usr/local/lib/python3.11/dist-packages (1.11.1)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.178.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.14.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio_client) (2025.3.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio_client) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from gradio_client) (0.34.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio_client) (25.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio_client) (15.0.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio_client) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio_client) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio_client) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio_client) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio_client) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.3->gradio_client) (3.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.3->gradio_client) (6.0.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.3->gradio_client) (1.1.7)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.24.1->gradio_client) (1.3.1)\n",
            "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install google-generativeai gradio_client python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "TOdqlf9vMCSy",
        "outputId": "28cf0825-ec61-4622-f06e-6c368fc3fb86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "你好！我是 Gemini，已經連接了您的字母計數器工具。您可以開始提問了。（輸入 'exit' 結束）\n",
            "您: 請問下列字串中初先過幾次y :  jjkashkdhsjkafhkahfksjyyfasyasyyasdy\n",
            "--- Gemini 決定呼叫工具: call_letter_counter_tool ---\n",
            "--- 正在呼叫遠端 MCP 伺服器 ---\n",
            "    伺服器: https://cwadayi-mcp-1.hf.space/\n",
            "    參數: word='jjkashkdhsjkafhkahfksjyyfasyasyyasdy', letter='y'\n",
            "Loaded as API: https://cwadayi-mcp-1.hf.space/ ✔\n",
            "--- MCP 伺服器回傳結果: 6 ---\n",
            "--- 將工具結果回傳給 Gemini，讓它產生最終回覆 ---\n",
            "Gemini: 有6個y。\n",
            "\n",
            "您: exit\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from gradio_client import Client\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# --- 1. 初始化與設定 ---\n",
        "load_dotenv()\n",
        "genai.configure(api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "MCP_SERVER_URL = \"https://cwadayi-mcp-1.hf.space/\"\n",
        "\n",
        "# --- 2. 「轉接器」函式 ---\n",
        "def call_mcp_letter_counter(word: str, letter: str) -> int:\n",
        "    \"\"\"\n",
        "    連接到遠端的 Gradio MCP 伺服器並執行 letter_counter 工具。\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"--- 正在呼叫遠端 MCP 伺服器 ---\")\n",
        "        print(f\"    伺服器: {MCP_SERVER_URL}\")\n",
        "        print(f\"    參數: word='{word}', letter='{letter}'\")\n",
        "\n",
        "        client = Client(src=MCP_SERVER_URL)\n",
        "\n",
        "        #    <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "        #    <<<<< 唯一的修改在這裡 >>>>\n",
        "        #    <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "        # 移除 api_name，讓 client 自動呼叫預設的端點\n",
        "        result = client.predict(word, letter)\n",
        "\n",
        "        print(f\"--- MCP 伺服器回傳結果: {result} ---\")\n",
        "        return int(result)\n",
        "    except Exception as e:\n",
        "        # 將更詳細的錯誤訊息印出，方便除錯\n",
        "        print(f\"呼叫 MCP 伺服器失敗: {e}\")\n",
        "        return -1\n",
        "\n",
        "# --- 3. 向 Gemini 定義可用的工具 ---\n",
        "letter_counter_tool_declaration = {\n",
        "    \"name\": \"call_letter_counter_tool\",\n",
        "    \"description\": \"計算一個指定的字母在一段文字中出現了幾次。\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"OBJECT\",\n",
        "        \"properties\": {\n",
        "            \"word\": { \"type\": \"STRING\", \"description\": \"要在其中搜尋的完整文字。\" },\n",
        "            \"letter\": { \"type\": \"STRING\", \"description\": \"要計數的單一字元。\" }\n",
        "        },\n",
        "        \"required\": [\"word\", \"letter\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "# --- 4. 建立 Gemini 模型 ---\n",
        "model = genai.GenerativeModel(\n",
        "    model_name='gemini-1.5-flash-latest',\n",
        "    tools=[letter_counter_tool_declaration]\n",
        ")\n",
        "\n",
        "# --- 5. 主執行迴圈 ---\n",
        "print(\"你好！我是 Gemini，已經連接了您的字母計數器工具。您可以開始提問了。（輸入 'exit' 結束）\")\n",
        "chat = model.start_chat(enable_automatic_function_calling=False)\n",
        "\n",
        "while True:\n",
        "    prompt = input(\"您: \")\n",
        "    if prompt.lower() == 'exit':\n",
        "        break\n",
        "\n",
        "    response = chat.send_message(prompt)\n",
        "\n",
        "    function_call = response.candidates[0].content.parts[0].function_call\n",
        "    if function_call:\n",
        "        print(f\"--- Gemini 決定呼叫工具: {function_call.name} ---\")\n",
        "\n",
        "        if function_call.name == \"call_letter_counter_tool\":\n",
        "            args = function_call.args\n",
        "            tool_result = call_mcp_letter_counter(word=args['word'], letter=args['letter'])\n",
        "\n",
        "            print(\"--- 將工具結果回傳給 Gemini，讓它產生最終回覆 ---\")\n",
        "            response = chat.send_message(\n",
        "                genai.protos.Part(\n",
        "                    function_response={\n",
        "                        \"name\": \"call_letter_counter_tool\",\n",
        "                        \"response\": {\"result\": tool_result},\n",
        "                    }\n",
        "                )\n",
        "            )\n",
        "            print(f\"Gemini: {response.text}\")\n",
        "    else:\n",
        "        print(f\"Gemini: {response.text}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import google.generativeai as genai\n",
        "from gradio_client import Client\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import userdata # 假設在 Colab 環境\n",
        "from datetime import datetime\n",
        "\n",
        "# --- 1. 初始化與設定 ---\n",
        "# 載入環境變數 (如果您的 API key 儲存在 .env 檔案中)\n",
        "# load_dotenv()\n",
        "# genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))\n",
        "\n",
        "# 直接從 Colab secrets 讀取\n",
        "genai.configure(api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "\n",
        "# 更新為新的 MCP-2 伺服器位址\n",
        "# MCP_SERVER_URL = \"cwadayi/MCP-2\"\n",
        "MCP_SERVER_URL = \"https://cwadayi-mcp-2.hf.space\"\n",
        "# --- 2. 新的「轉接器」函式 (用於地震查詢) ---\n",
        "def call_mcp_earthquake_search(\n",
        "    start_date: str,\n",
        "    end_date: str,\n",
        "    min_magnitude: float = 4.5,\n",
        "    max_magnitude: float = 8.0,\n",
        "    lat_from: float = 21.0,\n",
        "    lat_to: float = 26.0,\n",
        "    lon_from: float = 119.0,\n",
        "    lon_to: float = 123.0,\n",
        "    depth_from: float = 0.0,\n",
        "    depth_to: float = 100.0,\n",
        "    start_time: str = \"00:00:00\",\n",
        "    end_time: str = \"23:59:59\"\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    連接到遠端的 Gradio MCP 伺服器並執行地震資料查詢。\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"--- 正在呼叫遠端地震 MCP 伺服器 ---\")\n",
        "        print(f\"    伺服器: {MCP_SERVER_URL}\")\n",
        "        print(f\"    查詢條件: {start_date} 到 {end_date}, 規模 {min_magnitude} 以上\")\n",
        "\n",
        "        client = Client(src=MCP_SERVER_URL)\n",
        "\n",
        "        # 根據 MCP-2 的 API 文件，呼叫 `/gradio_fetch_and_plot_data`\n",
        "        # 並按照順序傳入 12 個參數\n",
        "        result = client.predict(\n",
        "            param_0=start_date,      # Start Date\n",
        "            param_1=start_time,      # Start Time\n",
        "            param_2=end_date,        # End Date\n",
        "            param_3=end_time,        # End Time\n",
        "            param_4=lat_from,        # Latitude From\n",
        "            param_5=lat_to,          # To\n",
        "            param_6=lon_from,        # Longitude From\n",
        "            param_7=lon_to,          # To\n",
        "            param_8=depth_from,      # Depth From\n",
        "            param_9=depth_to,        # To\n",
        "            param_10=min_magnitude,  # Magnitude From\n",
        "            param_11=max_magnitude,  # To\n",
        "            api_name=\"/gradio_fetch_and_plot_data\"\n",
        "        )\n",
        "\n",
        "        # API 回傳一個包含兩個元素的 tuple: (dataframe_dict, plot_dict)\n",
        "        # 我們需要的是第一個元素的資料部分\n",
        "        dataframe_dict = result[0]\n",
        "        headers = dataframe_dict.get('headers', [])\n",
        "        data = dataframe_dict.get('data', [])\n",
        "\n",
        "        if not data:\n",
        "            print(\"--- MCP 伺服器回傳：未找到符合條件的地震 ---\")\n",
        "            return \"查詢完成，但未找到任何符合條件的地震資料。\"\n",
        "\n",
        "        # 將回傳的資料轉換為更易讀的 JSON 格式\n",
        "        formatted_results = [dict(zip(headers, row)) for row in data]\n",
        "        json_result = json.dumps(formatted_results, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"--- MCP 伺服器成功回傳 {len(data)} 筆資料 ---\")\n",
        "        return json_result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"呼叫 MCP 伺服器失敗: {e}\")\n",
        "        return f\"工具執行失敗，錯誤訊息: {e}\"\n",
        "\n",
        "\n",
        "# --- 3. 向 Gemini 定義新的地震查詢工具 ---\n",
        "earthquake_search_tool_declaration = {\n",
        "    \"name\": \"call_earthquake_search_tool\",\n",
        "    \"description\": \"根據指定的條件（時間、地點、規模等）從台灣中央氣象署的資料庫中搜尋地震事件。預設搜尋台灣周邊地區。\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"OBJECT\",\n",
        "        \"properties\": {\n",
        "            \"start_date\": {\n",
        "                \"type\": \"STRING\",\n",
        "                \"description\": \"搜尋的開始日期，格式為 'YYYY-MM-DD'。例如：'2024-01-01'。\"\n",
        "            },\n",
        "            \"end_date\": {\n",
        "                \"type\": \"STRING\",\n",
        "                \"description\": f\"搜尋的結束日期，格式為 'YYYY-MM-DD'。例如：'2024-07-31'。預設為今天: {datetime.now().strftime('%Y-%m-%d')}。\"\n",
        "            },\n",
        "            \"min_magnitude\": {\n",
        "                \"type\": \"NUMBER\",\n",
        "                \"description\": \"要搜尋的最小地震規模。預設為 4.5。\"\n",
        "            },\n",
        "            \"max_magnitude\": {\n",
        "                \"type\": \"NUMBER\",\n",
        "                \"description\": \"要搜尋的最大地震規模。預設為 8.0。\"\n",
        "            },\n",
        "            \"lat_from\": {\"type\": \"NUMBER\", \"description\": \"緯度範圍的起始值。預設為 21.0。\"},\n",
        "            \"lat_to\": {\"type\": \"NUMBER\", \"description\": \"緯度範圍的結束值。預設為 26.0。\"},\n",
        "            \"lon_from\": {\"type\": \"NUMBER\", \"description\": \"經度範圍的起始值。預設為 119.0。\"},\n",
        "            \"lon_to\": {\"type\": \"NUMBER\", \"description\": \"經度範圍的結束值。預設為 123.0。\"},\n",
        "        },\n",
        "        \"required\": [\"start_date\", \"end_date\", \"min_magnitude\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "# --- 4. 建立 Gemini 模型 ---\n",
        "# 將 tools 參數更新為新的工具定義\n",
        "model = genai.GenerativeModel(\n",
        "    model_name='gemini-1.5-flash-latest',\n",
        "    tools=[earthquake_search_tool_declaration]\n",
        ")\n",
        "\n",
        "\n",
        "# --- 5. 主執行迴圈 ---\n",
        "print(\"你好！我是 Gemini，已經連接了您的地震查詢工具 🗺️。您可以開始提問了。（輸入 'exit' 結束）\")\n",
        "chat = model.start_chat(enable_automatic_function_calling=False) # 我們手動處理 Function Call\n",
        "\n",
        "while True:\n",
        "    prompt = input(\"您: \")\n",
        "    if prompt.lower() == 'exit':\n",
        "        break\n",
        "\n",
        "    try:\n",
        "        response = chat.send_message(prompt)\n",
        "        part = response.candidates[0].content.parts[0]\n",
        "\n",
        "        if part.function_call:\n",
        "            function_call = part.function_call\n",
        "            print(f\"--- Gemini 決定呼叫工具: {function_call.name} ---\")\n",
        "\n",
        "            if function_call.name == \"call_earthquake_search_tool\":\n",
        "                args = function_call.args\n",
        "                # 確保 end_date 有預設值\n",
        "                if 'end_date' not in args:\n",
        "                    args['end_date'] = datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "                tool_result = call_mcp_earthquake_search(**args) # 使用 **args 將字典解包為關鍵字參數\n",
        "\n",
        "                print(\"--- 將工具結果回傳給 Gemini，讓它產生最終回覆 ---\")\n",
        "                response = chat.send_message(\n",
        "                    genai.protos.Part(\n",
        "                        function_response={\n",
        "                            \"name\": \"call_earthquake_search_tool\",\n",
        "                            \"response\": {\"result\": tool_result},\n",
        "                        }\n",
        "                    )\n",
        "                )\n",
        "                print(f\"Gemini: {response.text}\")\n",
        "        else:\n",
        "            print(f\"Gemini: {response.text}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"發生錯誤：{e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "Op98oGe76s74",
        "outputId": "6cee067e-e6e9-4bf4-ea1a-f15e68117a0d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "你好！我是 Gemini，已經連接了您的地震查詢工具 🗺️。您可以開始提問了。（輸入 'exit' 結束）\n",
            "您: what is the largest earthquake in 2024 ? show lon,lat,mag,dep,time\n",
            "--- Gemini 決定呼叫工具: call_earthquake_search_tool ---\n",
            "--- 正在呼叫遠端地震 MCP 伺服器 ---\n",
            "    伺服器: https://cwadayi-mcp-2.hf.space\n",
            "    查詢條件: 2024-01-01 到 2024-12-31, 規模 0.0 以上\n",
            "Loaded as API: https://cwadayi-mcp-2.hf.space/ ✔\n",
            "--- MCP 伺服器成功回傳 1000 筆資料 ---\n",
            "--- 將工具結果回傳給 Gemini，讓它產生最終回覆 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-1.5-flash-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 46951.54ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-1.5-flash-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 5569.23ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-1.5-flash-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 5014.81ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-1.5-flash-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1503.26ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "發生錯誤：429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1348.84ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "您: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import google.generativeai as genai\n",
        "from gradio_client import Client\n",
        "from dotenv import load_dotenv\n",
        "from datetime import datetime\n",
        "\n",
        "# --- Colab 使用者請注意 ---\n",
        "# 如果您在 Google Colab 中執行，請使用以下程式碼來載入您的 API Key。\n",
        "# 在左側面板點擊鑰匙圖示 (Secrets)，新增一個名為 'GOOGLE_API_KEY' 的 secret，\n",
        "# 並將您的 API Key 貼入。\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "# --- 本地端使用者請注意 ---\n",
        "# 如果您在本地端執行，請確保您的 .env 檔案中有 'GOOGLE_API_KEY=\"您的金鑰\"'\n",
        "except (ImportError, ModuleNotFoundError):\n",
        "    load_dotenv()\n",
        "    GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
        "\n",
        "# --- 1. 初始化與設定 ---\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# 地震資料查詢 MCP 伺服器位址\n",
        "MCP_SERVER_URL = \"https://cwadayi-mcp-2.hf.space\"\n",
        "\n",
        "\n",
        "# --- 2. 「轉接器」函式 (用於地震查詢) ---\n",
        "def call_mcp_earthquake_search(\n",
        "    start_date: str,\n",
        "    end_date: str,\n",
        "    min_magnitude: float = 4.0,\n",
        "    max_magnitude: float = 9.0,\n",
        "    lat_from: float = 21.0,\n",
        "    lat_to: float = 26.0,\n",
        "    lon_from: float = 119.0,\n",
        "    lon_to: float = 123.0,\n",
        "    depth_from: float = 0.0,\n",
        "    depth_to: float = 100.0,\n",
        "    start_time: str = \"00:00:00\",\n",
        "    end_time: str = \"23:59:59\"\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    連接到遠端的 Gradio MCP 伺服器並執行地震資料查詢。\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"--- 正在呼叫遠端地震 MCP 伺服器 ---\")\n",
        "        print(f\"    伺服器: {MCP_SERVER_URL}\")\n",
        "        print(f\"    查詢條件: {start_date} 到 {end_date}, 規模 {min_magnitude} 以上\")\n",
        "\n",
        "        client = Client(src=MCP_SERVER_URL)\n",
        "\n",
        "        # 根據 MCP-2 的 API 文件，呼叫 `/gradio_fetch_and_plot_data`\n",
        "        # 並按照順序傳入 12 個參數\n",
        "        result = client.predict(\n",
        "            param_0=start_date,      # Start Date\n",
        "            param_1=start_time,      # Start Time\n",
        "            param_2=end_date,        # End Date\n",
        "            param_3=end_time,        # End Time\n",
        "            param_4=lat_from,        # Latitude From\n",
        "            param_5=lat_to,          # To\n",
        "            param_6=lon_from,        # Longitude From\n",
        "            param_7=lon_to,          # To\n",
        "            param_8=depth_from,      # Depth From\n",
        "            param_9=depth_to,        # To\n",
        "            param_10=min_magnitude,  # Magnitude From\n",
        "            param_11=max_magnitude,  # To\n",
        "            api_name=\"/gradio_fetch_and_plot_data\"\n",
        "        )\n",
        "\n",
        "        # API 回傳一個包含兩個元素的 tuple: (dataframe_dict, plot_dict)\n",
        "        # 我們需要的是第一個元素的資料部分\n",
        "        dataframe_dict = result[0]\n",
        "        headers = dataframe_dict.get('headers', [])\n",
        "        data = dataframe_dict.get('data', [])\n",
        "\n",
        "        if not data:\n",
        "            print(\"--- MCP 伺服器回傳：未找到符合條件的地震 ---\")\n",
        "            return \"查詢完成，但未找到任何符合條件的地震資料。\"\n",
        "\n",
        "        # 將回傳的資料轉換為更易讀的 JSON 格式\n",
        "        formatted_results = [dict(zip(headers, row)) for row in data]\n",
        "        json_result = json.dumps(formatted_results, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"--- MCP 伺服器成功回傳 {len(data)} 筆資料 ---\")\n",
        "        return json_result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"呼叫 MCP 伺服器失敗: {e}\")\n",
        "        return f\"工具執行失敗，錯誤訊息: {e}\"\n",
        "\n",
        "\n",
        "# --- 3. 向 Gemini 定義新的地震查詢工具 ---\n",
        "earthquake_search_tool_declaration = {\n",
        "    \"name\": \"call_earthquake_search_tool\",\n",
        "    \"description\": \"根據指定的條件（時間、地點、規模等）從台灣中央氣象署的資料庫中搜尋地震事件。預設搜尋台灣周邊地區。\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"OBJECT\",\n",
        "        \"properties\": {\n",
        "            \"start_date\": {\n",
        "                \"type\": \"STRING\",\n",
        "                \"description\": \"搜尋的開始日期，格式為 'YYYY-MM-DD'。例如：'2025-01-01'。\"\n",
        "            },\n",
        "            \"end_date\": {\n",
        "                \"type\": \"STRING\",\n",
        "                \"description\": f\"搜尋的結束日期，格式為 'YYYY-MM-DD'。例如：'2025-08-14'。預設為今天。\"\n",
        "            },\n",
        "            \"min_magnitude\": {\n",
        "                \"type\": \"NUMBER\",\n",
        "                \"description\": \"要搜尋的最小地震規模。預設為 4.0。\"\n",
        "            },\n",
        "            \"max_magnitude\": {\n",
        "                \"type\": \"NUMBER\",\n",
        "                \"description\": \"要搜尋的最大地震規模。預設為 9.0。\"\n",
        "            },\n",
        "            \"lat_from\": {\"type\": \"NUMBER\", \"description\": \"緯度範圍的起始值。預設為 21.0。\"},\n",
        "            \"lat_to\": {\"type\": \"NUMBER\", \"description\": \"緯度範圍的結束值。預設為 26.0。\"},\n",
        "            \"lon_from\": {\"type\": \"NUMBER\", \"description\": \"經度範圍的起始值。預設為 119.0。\"},\n",
        "            \"lon_to\": {\"type\": \"NUMBER\", \"description\": \"經度範圍的結束值。預設為 123.0。\"},\n",
        "        },\n",
        "        \"required\": [\"start_date\", \"end_date\"] # 讓 Gemini 必須提供日期\n",
        "    }\n",
        "}\n",
        "\n",
        "# --- 4. 建立 Gemini 模型 ---\n",
        "model = genai.GenerativeModel(\n",
        "    model_name='gemini-1.5-flash-latest',\n",
        "    tools=[earthquake_search_tool_declaration]\n",
        ")\n",
        "\n",
        "\n",
        "# --- 5. 主執行迴圈 (修正後版本) ---\n",
        "print(\"你好！我是 Gemini，已經連接了您的地震查詢工具 🗺️。您可以開始提問了。（輸入 'exit' 結束）\")\n",
        "chat = model.start_chat(enable_automatic_function_calling=False) # 我們手動處理 Function Call\n",
        "\n",
        "while True:\n",
        "    prompt = input(\"您: \")\n",
        "    if prompt.lower() == 'exit':\n",
        "        break\n",
        "\n",
        "    try:\n",
        "        response = chat.send_message(prompt)\n",
        "\n",
        "        # --- 穩健的回應處理邏輯 ---\n",
        "        has_function_call = False\n",
        "        text_response = \"\"\n",
        "\n",
        "        for part in response.candidates[0].content.parts:\n",
        "            if part.function_call:\n",
        "                function_call = part.function_call\n",
        "                print(f\"--- Gemini 決定呼叫工具: {function_call.name} ---\")\n",
        "\n",
        "                if function_call.name == \"call_earthquake_search_tool\":\n",
        "                    has_function_call = True\n",
        "                    args = function_call.args\n",
        "                    # 如果使用者沒提供結束日期，自動設為今天\n",
        "                    if 'end_date' not in args:\n",
        "                        args['end_date'] = datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "                    tool_result = call_mcp_earthquake_search(**args)\n",
        "\n",
        "                    print(\"--- 將工具結果回傳給 Gemini，讓它產生最終回覆 ---\")\n",
        "                    # 將工具的執行結果回傳給模型\n",
        "                    response = chat.send_message(\n",
        "                        genai.protos.Part(\n",
        "                            function_response={\n",
        "                                \"name\": \"call_earthquake_search_tool\",\n",
        "                                \"response\": {\"result\": tool_result},\n",
        "                            }\n",
        "                        )\n",
        "                    )\n",
        "                    # 處理工具執行後的最終回覆\n",
        "                    for final_part in response.candidates[0].content.parts:\n",
        "                        text_response += final_part.text\n",
        "\n",
        "            elif part.text:\n",
        "                 text_response += part.text\n",
        "\n",
        "        # 迴圈結束後，印出收集到的所有文字回應\n",
        "        if text_response:\n",
        "             print(f\"Gemini: {text_response}\")\n",
        "        # 如果模型沒有產生任何文字，也沒有呼叫函式，給一個通用回覆\n",
        "        elif not has_function_call:\n",
        "             print(\"Gemini: 我不確定該如何回應。\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"發生錯誤：{e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "ngEThueh_wVh",
        "outputId": "0bccd337-3e21-4ecb-9840-8a5d501ec8c7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "你好！我是 Gemini，已經連接了您的地震查詢工具 🗺️。您可以開始提問了。（輸入 'exit' 結束）\n",
            "您: what is the largest earthquake in 2024 ?\n",
            "Gemini: I need to know the available data to answer your question.  The available tools don't specify how to get the largest earthquake within a year.  Can you provide more information or different tools?\n",
            "\n",
            "您: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O_uaRbQb_vte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "id": "hbwK1PfKM1gw",
        "outputId": "b98ef17d-0f39-42be-b63c-0e51717c2d21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在查詢您帳號可用的模型...\n",
            "\n",
            "-> 可用模型: models/gemini-1.5-pro-latest\n",
            "-> 可用模型: models/gemini-1.5-pro-002\n",
            "-> 可用模型: models/gemini-1.5-pro\n",
            "-> 可用模型: models/gemini-1.5-flash-latest\n",
            "-> 可用模型: models/gemini-1.5-flash\n",
            "-> 可用模型: models/gemini-1.5-flash-002\n",
            "-> 可用模型: models/gemini-1.5-flash-8b\n",
            "-> 可用模型: models/gemini-1.5-flash-8b-001\n",
            "-> 可用模型: models/gemini-1.5-flash-8b-latest\n",
            "-> 可用模型: models/gemini-2.5-pro-preview-03-25\n",
            "-> 可用模型: models/gemini-2.5-flash-preview-05-20\n",
            "-> 可用模型: models/gemini-2.5-flash\n",
            "-> 可用模型: models/gemini-2.5-flash-lite-preview-06-17\n",
            "-> 可用模型: models/gemini-2.5-pro-preview-05-06\n",
            "-> 可用模型: models/gemini-2.5-pro-preview-06-05\n",
            "-> 可用模型: models/gemini-2.5-pro\n",
            "-> 可用模型: models/gemini-2.0-flash-exp\n",
            "-> 可用模型: models/gemini-2.0-flash\n",
            "-> 可用模型: models/gemini-2.0-flash-001\n",
            "-> 可用模型: models/gemini-2.0-flash-exp-image-generation\n",
            "-> 可用模型: models/gemini-2.0-flash-lite-001\n",
            "-> 可用模型: models/gemini-2.0-flash-lite\n",
            "-> 可用模型: models/gemini-2.0-flash-preview-image-generation\n",
            "-> 可用模型: models/gemini-2.0-flash-lite-preview-02-05\n",
            "-> 可用模型: models/gemini-2.0-flash-lite-preview\n",
            "-> 可用模型: models/gemini-2.0-pro-exp\n",
            "-> 可用模型: models/gemini-2.0-pro-exp-02-05\n",
            "-> 可用模型: models/gemini-exp-1206\n",
            "-> 可用模型: models/gemini-2.0-flash-thinking-exp-01-21\n",
            "-> 可用模型: models/gemini-2.0-flash-thinking-exp\n",
            "-> 可用模型: models/gemini-2.0-flash-thinking-exp-1219\n",
            "-> 可用模型: models/gemini-2.5-flash-preview-tts\n",
            "-> 可用模型: models/gemini-2.5-pro-preview-tts\n",
            "-> 可用模型: models/learnlm-2.0-flash-experimental\n",
            "-> 可用模型: models/gemma-3-1b-it\n",
            "-> 可用模型: models/gemma-3-4b-it\n",
            "-> 可用模型: models/gemma-3-12b-it\n",
            "-> 可用模型: models/gemma-3-27b-it\n",
            "-> 可用模型: models/gemma-3n-e4b-it\n",
            "-> 可用模型: models/gemma-3n-e2b-it\n",
            "-> 可用模型: models/gemini-2.5-flash-lite\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# 載入您的 API 金鑰\n",
        "load_dotenv()\n",
        "genai.configure(api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "\n",
        "print(\"正在查詢您帳號可用的模型...\\n\")\n",
        "\n",
        "# 遍歷所有模型\n",
        "for model in genai.list_models():\n",
        "  # 檢查該模型是否支援 'generateContent' 方法（聊天和生成內容所必需的）\n",
        "  if 'generateContent' in model.supported_generation_methods:\n",
        "    print(f\"-> 可用模型: {model.name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "8NgC6X5wPZ1t",
        "outputId": "6d071b0c-cb19-47f0-c5ca-5d37bb8e7dc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "你好！我是 Gemini，已經連接了您的字母計數器工具。您可以開始提問了。（輸入 'exit' 結束）\n",
            "您: 2015年以後規模大於5.0的地震有幾個\n",
            "Gemini: I do not have access to real-time information, including earthquake data.  Therefore, I cannot answer how many earthquakes larger than magnitude 5.0 occurred after 2015.  To find this information, you would need to consult a seismic database such as those maintained by the USGS (United States Geological Survey) or other similar organizations.\n",
            "\n",
            "您: exit\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from gradio_client import Client\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# --- 1. 初始化與設定 ---\n",
        "load_dotenv()\n",
        "genai.configure(api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "MCP_SERVER_URL = \"https://cwadayi-mcp-2.hf.space/\"\n",
        "\n",
        "# --- 2. 「轉接器」函式 ---\n",
        "def call_mcp_letter_counter(word: str, letter: str) -> int:\n",
        "    \"\"\"\n",
        "    連接到遠端的 Gradio MCP 伺服器並執行 letter_counter 工具。\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"--- 正在呼叫遠端 MCP 伺服器 ---\")\n",
        "        print(f\"    伺服器: {MCP_SERVER_URL}\")\n",
        "        print(f\"    參數: word='{word}', letter='{letter}'\")\n",
        "\n",
        "        client = Client(src=MCP_SERVER_URL)\n",
        "\n",
        "        #    <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "        #    <<<<< 唯一的修改在這裡 >>>>\n",
        "        #    <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "        # 移除 api_name，讓 client 自動呼叫預設的端點\n",
        "        result = client.predict(word, letter)\n",
        "\n",
        "        print(f\"--- MCP 伺服器回傳結果: {result} ---\")\n",
        "        return int(result)\n",
        "    except Exception as e:\n",
        "        # 將更詳細的錯誤訊息印出，方便除錯\n",
        "        print(f\"呼叫 MCP 伺服器失敗: {e}\")\n",
        "        return -1\n",
        "\n",
        "# --- 3. 向 Gemini 定義可用的工具 ---\n",
        "letter_counter_tool_declaration = {\n",
        "    \"name\": \"call_letter_counter_tool\",\n",
        "    \"description\": \"計算一個指定的字母在一段文字中出現了幾次。\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"OBJECT\",\n",
        "        \"properties\": {\n",
        "            \"word\": { \"type\": \"STRING\", \"description\": \"要在其中搜尋的完整文字。\" },\n",
        "            \"letter\": { \"type\": \"STRING\", \"description\": \"要計數的單一字元。\" }\n",
        "        },\n",
        "        \"required\": [\"word\", \"letter\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "# --- 4. 建立 Gemini 模型 ---\n",
        "model = genai.GenerativeModel(\n",
        "    model_name='gemini-1.5-flash-latest',\n",
        "    tools=[letter_counter_tool_declaration]\n",
        ")\n",
        "\n",
        "# --- 5. 主執行迴圈 ---\n",
        "print(\"你好！我是 Gemini，已經連接了您的字母計數器工具。您可以開始提問了。（輸入 'exit' 結束）\")\n",
        "chat = model.start_chat(enable_automatic_function_calling=False)\n",
        "\n",
        "while True:\n",
        "    prompt = input(\"您: \")\n",
        "    if prompt.lower() == 'exit':\n",
        "        break\n",
        "\n",
        "    response = chat.send_message(prompt)\n",
        "\n",
        "    function_call = response.candidates[0].content.parts[0].function_call\n",
        "    if function_call:\n",
        "        print(f\"--- Gemini 決定呼叫工具: {function_call.name} ---\")\n",
        "\n",
        "        if function_call.name == \"call_letter_counter_tool\":\n",
        "            args = function_call.args\n",
        "            tool_result = call_mcp_letter_counter(word=args['word'], letter=args['letter'])\n",
        "\n",
        "            print(\"--- 將工具結果回傳給 Gemini，讓它產生最終回覆 ---\")\n",
        "            response = chat.send_message(\n",
        "                genai.protos.Part(\n",
        "                    function_response={\n",
        "                        \"name\": \"call_letter_counter_tool\",\n",
        "                        \"response\": {\"result\": tool_result},\n",
        "                    }\n",
        "                )\n",
        "            )\n",
        "            print(f\"Gemini: {response.text}\")\n",
        "    else:\n",
        "        print(f\"Gemini: {response.text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKaaS5THPce5"
      },
      "outputs": [],
      "source": [
        "api_key = userdata.get('GOOGLE_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QfKBNR4NAd-",
        "outputId": "fa9d7593-e157-49c1-88f0-5d0e5eb1e525"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:__main__:Failed to initialize: HTTP 404\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to connect to MCP server\n",
            "\n",
            "============================================================\n",
            "EVENT LOOP ISSUE DETECTED\n",
            "============================================================\n",
            "If you're in Jupyter notebook, use these commands instead:\n",
            "\n",
            "# Set your API key\n",
            "api_key = 'your-gemini-api-key'\n",
            "\n",
            "# Quick analysis\n",
            "analysis = await quick_analysis(api_key)\n",
            "print(analysis)\n",
            "\n",
            "# Ask a question\n",
            "answer = await ask_earthquake_question(api_key, 'What was the strongest earthquake?')\n",
            "print(answer)\n",
            "\n",
            "# Create analyst for multiple queries\n",
            "analyst = await create_analyst(api_key)\n",
            "if analyst:\n",
            "    result = await analyst.analyze_earthquake_data({'ML_min': 5.0})\n",
            "    print(result)\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Google Gemini MCP Client for Earthquake Data Server\n",
        "Connects to the earthquake data MCP server deployed on Hugging Face Spaces\n",
        "and uses Gemini AI to provide intelligent earthquake data analysis.\n",
        "\"\"\"\n",
        "\n",
        "import asyncio\n",
        "import json\n",
        "import logging\n",
        "import aiohttp\n",
        "import google.generativeai as genai\n",
        "from typing import Dict, Any, List, Optional\n",
        "from datetime import datetime\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@dataclass\n",
        "class MCPResponse:\n",
        "    \"\"\"Data class for MCP response\"\"\"\n",
        "    success: bool\n",
        "    data: Optional[Dict[str, Any]] = None\n",
        "    error: Optional[str] = None\n",
        "\n",
        "class EarthquakeMCPClient:\n",
        "    \"\"\"MCP Client for connecting to earthquake data server\"\"\"\n",
        "\n",
        "    def __init__(self, server_url: str):\n",
        "        self.server_url = server_url.rstrip('/')\n",
        "        self.mcp_endpoint = f\"{self.server_url}/mcp\"  # Assuming MCP endpoint\n",
        "        self.session = None\n",
        "        self.request_id = 0\n",
        "\n",
        "    async def __aenter__(self):\n",
        "        \"\"\"Async context manager entry\"\"\"\n",
        "        self.session = aiohttp.ClientSession()\n",
        "        return self\n",
        "\n",
        "    async def __aexit__(self, exc_type, exc_val, exc_tb):\n",
        "        \"\"\"Async context manager exit\"\"\"\n",
        "        if self.session:\n",
        "            await self.session.close()\n",
        "\n",
        "    def _get_next_id(self) -> int:\n",
        "        \"\"\"Get next request ID\"\"\"\n",
        "        self.request_id += 1\n",
        "        return self.request_id\n",
        "\n",
        "    async def _make_mcp_request(self, method: str, params: Dict[str, Any] = None) -> MCPResponse:\n",
        "        \"\"\"Make MCP request to the server\"\"\"\n",
        "        if not self.session:\n",
        "            raise RuntimeError(\"Client not initialized. Use async context manager.\")\n",
        "\n",
        "        request_data = {\n",
        "            \"jsonrpc\": \"2.0\",\n",
        "            \"id\": self._get_next_id(),\n",
        "            \"method\": method,\n",
        "            \"params\": params or {}\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Try direct HTTP request to Gradio MCP interface\n",
        "            async with self.session.post(\n",
        "                f\"{self.server_url}/api/predict\",\n",
        "                json={\n",
        "                    \"data\": [json.dumps(request_data)],\n",
        "                    \"fn_index\": 1  # Assuming MCP handler is at index 1\n",
        "                },\n",
        "                headers={\"Content-Type\": \"application/json\"},\n",
        "                timeout=30\n",
        "            ) as response:\n",
        "                if response.status == 200:\n",
        "                    result = await response.json()\n",
        "                    # Extract response from Gradio format\n",
        "                    if \"data\" in result and result[\"data\"]:\n",
        "                        mcp_response = json.loads(result[\"data\"][0])\n",
        "                        if \"error\" in mcp_response:\n",
        "                            return MCPResponse(success=False, error=mcp_response[\"error\"][\"message\"])\n",
        "                        return MCPResponse(success=True, data=mcp_response.get(\"result\"))\n",
        "                    else:\n",
        "                        return MCPResponse(success=False, error=\"Invalid response format\")\n",
        "                else:\n",
        "                    return MCPResponse(success=False, error=f\"HTTP {response.status}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"MCP request failed: {e}\")\n",
        "            return MCPResponse(success=False, error=str(e))\n",
        "\n",
        "    async def initialize(self) -> MCPResponse:\n",
        "        \"\"\"Initialize MCP connection\"\"\"\n",
        "        return await self._make_mcp_request(\"initialize\", {\n",
        "            \"protocolVersion\": \"2024-11-05\",\n",
        "            \"capabilities\": {},\n",
        "            \"clientInfo\": {\n",
        "                \"name\": \"gemini-mcp-client\",\n",
        "                \"version\": \"1.0.0\"\n",
        "            }\n",
        "        })\n",
        "\n",
        "    async def list_tools(self) -> MCPResponse:\n",
        "        \"\"\"List available tools\"\"\"\n",
        "        return await self._make_mcp_request(\"tools/list\")\n",
        "\n",
        "    async def call_tool(self, tool_name: str, arguments: Dict[str, Any] = None) -> MCPResponse:\n",
        "        \"\"\"Call a specific tool\"\"\"\n",
        "        return await self._make_mcp_request(\"tools/call\", {\n",
        "            \"name\": tool_name,\n",
        "            \"arguments\": arguments or {}\n",
        "        })\n",
        "\n",
        "class GeminiEarthquakeAnalyst:\n",
        "    \"\"\"Gemini AI analyst for earthquake data\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str, model_name: str = \"gemini-pro\"):\n",
        "        genai.configure(api_key=api_key)\n",
        "        self.model = genai.GenerativeModel(model_name)\n",
        "        self.mcp_client = None\n",
        "\n",
        "    async def connect_to_mcp_server(self, server_url: str):\n",
        "        \"\"\"Connect to MCP server\"\"\"\n",
        "        self.mcp_client = EarthquakeMCPClient(server_url)\n",
        "\n",
        "    async def initialize_connection(self) -> bool:\n",
        "        \"\"\"Initialize MCP connection\"\"\"\n",
        "        if not self.mcp_client:\n",
        "            logger.error(\"MCP client not connected\")\n",
        "            return False\n",
        "\n",
        "        async with self.mcp_client as client:\n",
        "            # Initialize connection\n",
        "            init_response = await client.initialize()\n",
        "            if not init_response.success:\n",
        "                logger.error(f\"Failed to initialize: {init_response.error}\")\n",
        "                return False\n",
        "\n",
        "            # List available tools\n",
        "            tools_response = await client.list_tools()\n",
        "            if not tools_response.success:\n",
        "                logger.error(f\"Failed to list tools: {tools_response.error}\")\n",
        "                return False\n",
        "\n",
        "            logger.info(\"Successfully connected to MCP server\")\n",
        "            logger.info(f\"Available tools: {list(tools_response.data.get('tools', []))}\")\n",
        "            return True\n",
        "\n",
        "    async def query_earthquakes(self, **kwargs) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"Query earthquake data\"\"\"\n",
        "        async with self.mcp_client as client:\n",
        "            response = await client.call_tool(\"query_earthquakes\", kwargs)\n",
        "            if response.success:\n",
        "                return json.loads(response.data[\"content\"][0][\"text\"])\n",
        "            else:\n",
        "                logger.error(f\"Query failed: {response.error}\")\n",
        "                return None\n",
        "\n",
        "    async def get_earthquake_stats(self, **kwargs) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"Get earthquake statistics\"\"\"\n",
        "        async with self.mcp_client as client:\n",
        "            response = await client.call_tool(\"get_earthquake_stats\", kwargs)\n",
        "            if response.success:\n",
        "                return json.loads(response.data[\"content\"][0][\"text\"])\n",
        "            else:\n",
        "                logger.error(f\"Stats query failed: {response.error}\")\n",
        "                return None\n",
        "\n",
        "    async def create_earthquake_map(self, **kwargs) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"Create earthquake map\"\"\"\n",
        "        async with self.mcp_client as client:\n",
        "            response = await client.call_tool(\"create_earthquake_map\",\n",
        "                                            {**kwargs, \"return_base64\": True})\n",
        "            if response.success:\n",
        "                return json.loads(response.data[\"content\"][0][\"text\"])\n",
        "            else:\n",
        "                logger.error(f\"Map creation failed: {response.error}\")\n",
        "                return None\n",
        "\n",
        "    def _format_earthquake_data_for_gemini(self, data: Dict[str, Any]) -> str:\n",
        "        \"\"\"Format earthquake data for Gemini analysis\"\"\"\n",
        "        if not data.get(\"success\"):\n",
        "            return f\"Error retrieving data: {data.get('error', 'Unknown error')}\"\n",
        "\n",
        "        if data.get(\"count\", 0) == 0:\n",
        "            return \"No earthquake data found for the specified criteria.\"\n",
        "\n",
        "        formatted = f\"\"\"\n",
        "Earthquake Data Summary:\n",
        "- Total earthquakes: {data.get('count', 0)}\n",
        "- Date range: Query parameters used\n",
        "        \"\"\"\n",
        "\n",
        "        if \"summary\" in data:\n",
        "            summary = data[\"summary\"]\n",
        "            formatted += f\"\"\"\n",
        "- Magnitude range: {summary.get('magnitude_range', [])}\n",
        "- Depth range: {summary.get('depth_range', [])} km\n",
        "- Location bounds: {summary.get('location_bounds', {})}\n",
        "            \"\"\"\n",
        "\n",
        "        if \"data\" in data and data[\"data\"]:\n",
        "            formatted += f\"\\nSample earthquakes:\\n\"\n",
        "            for i, eq in enumerate(data[\"data\"][:5]):  # Show first 5\n",
        "                formatted += f\"- Date: {eq.get('date', 'N/A')}, Magnitude: {eq.get('ML', 'N/A')}, Depth: {eq.get('depth', 'N/A')}km\\n\"\n",
        "\n",
        "        return formatted.strip()\n",
        "\n",
        "    def _format_stats_for_gemini(self, stats: Dict[str, Any]) -> str:\n",
        "        \"\"\"Format statistics for Gemini analysis\"\"\"\n",
        "        if not stats.get(\"success\"):\n",
        "            return f\"Error retrieving statistics: {stats.get('error', 'Unknown error')}\"\n",
        "\n",
        "        formatted = f\"\"\"\n",
        "Earthquake Statistics:\n",
        "- Total earthquakes: {stats.get('total_earthquakes', 0)}\n",
        "- Date range: {stats.get('date_range', {})}\n",
        "\n",
        "Magnitude Statistics:\n",
        "- Range: {stats.get('magnitude_stats', {}).get('min', 'N/A')} - {stats.get('magnitude_stats', {}).get('max', 'N/A')}\n",
        "- Mean: {stats.get('magnitude_stats', {}).get('mean', 'N/A'):.2f}\n",
        "- Median: {stats.get('magnitude_stats', {}).get('median', 'N/A'):.2f}\n",
        "\n",
        "Depth Statistics:\n",
        "- Range: {stats.get('depth_stats', {}).get('min', 'N/A')} - {stats.get('depth_stats', {}).get('max', 'N/A')} km\n",
        "- Mean: {stats.get('depth_stats', {}).get('mean', 'N/A'):.2f} km\n",
        "\n",
        "Magnitude Distribution:\n",
        "{json.dumps(stats.get('magnitude_distribution', {}), indent=2)}\n",
        "\n",
        "Depth Distribution:\n",
        "{json.dumps(stats.get('depth_distribution', {}), indent=2)}\n",
        "        \"\"\"\n",
        "\n",
        "        return formatted.strip()\n",
        "\n",
        "    async def analyze_earthquake_data(self, query_params: Dict[str, Any] = None) -> str:\n",
        "        \"\"\"Analyze earthquake data using Gemini\"\"\"\n",
        "        try:\n",
        "            # Get earthquake data\n",
        "            data = await self.query_earthquakes(**(query_params or {}))\n",
        "            if not data:\n",
        "                return \"Failed to retrieve earthquake data.\"\n",
        "\n",
        "            # Get statistics\n",
        "            stats = await self.get_earthquake_stats(**(query_params or {}))\n",
        "\n",
        "            # Format data for Gemini\n",
        "            data_text = self._format_earthquake_data_for_gemini(data)\n",
        "            stats_text = self._format_stats_for_gemini(stats) if stats else \"\"\n",
        "\n",
        "            # Create prompt for Gemini\n",
        "            prompt = f\"\"\"\n",
        "Please analyze the following earthquake data and provide insights:\n",
        "\n",
        "{data_text}\n",
        "\n",
        "{stats_text}\n",
        "\n",
        "Please provide:\n",
        "1. A summary of the earthquake activity\n",
        "2. Notable patterns or trends\n",
        "3. Risk assessment based on the data\n",
        "4. Recommendations for further analysis\n",
        "5. Any significant observations about magnitude, depth, or location patterns\n",
        "\n",
        "Be specific and reference the actual data in your analysis.\n",
        "            \"\"\"\n",
        "\n",
        "            # Generate analysis using Gemini\n",
        "            response = self.model.generate_content(prompt)\n",
        "            return response.text\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Analysis failed: {e}\")\n",
        "            return f\"Analysis failed: {str(e)}\"\n",
        "\n",
        "    async def answer_earthquake_question(self, question: str,\n",
        "                                       query_params: Dict[str, Any] = None) -> str:\n",
        "        \"\"\"Answer specific questions about earthquake data\"\"\"\n",
        "        try:\n",
        "            # Get relevant data based on the question\n",
        "            data = await self.query_earthquakes(**(query_params or {}))\n",
        "            stats = await self.get_earthquake_stats(**(query_params or {}))\n",
        "\n",
        "            if not data:\n",
        "                return \"I couldn't retrieve earthquake data to answer your question.\"\n",
        "\n",
        "            # Format data for context\n",
        "            data_context = self._format_earthquake_data_for_gemini(data)\n",
        "            stats_context = self._format_stats_for_gemini(stats) if stats else \"\"\n",
        "\n",
        "            prompt = f\"\"\"\n",
        "Based on the following earthquake data, please answer this question: \"{question}\"\n",
        "\n",
        "Earthquake Data:\n",
        "{data_context}\n",
        "\n",
        "Statistics:\n",
        "{stats_context}\n",
        "\n",
        "Please provide a detailed, accurate answer based on the actual data provided. If the data doesn't contain enough information to answer the question, please say so.\n",
        "            \"\"\"\n",
        "\n",
        "            response = self.model.generate_content(prompt)\n",
        "            return response.text\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Question answering failed: {e}\")\n",
        "            return f\"I encountered an error: {str(e)}\"\n",
        "\n",
        "async def main():\n",
        "    \"\"\"Main function demonstrating the MCP client usage\"\"\"\n",
        "\n",
        "    # Configuration\n",
        "    GEMINI_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    MCP_SERVER_URL = \"https://cwadayi-mcp-2.hf.space\"\n",
        "\n",
        "    if not GEMINI_API_KEY:\n",
        "        print(\"Please set GEMINI_API_KEY environment variable\")\n",
        "        return\n",
        "\n",
        "    # Initialize Gemini analyst\n",
        "    analyst = GeminiEarthquakeAnalyst(GEMINI_API_KEY)\n",
        "    await analyst.connect_to_mcp_server(MCP_SERVER_URL)\n",
        "\n",
        "    # Test connection\n",
        "    if not await analyst.initialize_connection():\n",
        "        print(\"Failed to connect to MCP server\")\n",
        "        return\n",
        "\n",
        "    print(\"=== Gemini Earthquake Data Analyst ===\\n\")\n",
        "\n",
        "    # Example 1: General analysis\n",
        "    print(\"1. General earthquake analysis for Taiwan region (2024):\")\n",
        "    analysis = await analyst.analyze_earthquake_data({\n",
        "        \"start_date\": \"2024-01-01\",\n",
        "        \"end_date\": \"2024-12-31\",\n",
        "        \"ML_min\": 4.0\n",
        "    })\n",
        "    print(analysis)\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    # Example 2: Answer specific question\n",
        "    print(\"2. Answering specific question:\")\n",
        "    question = \"What was the strongest earthquake in the first quarter of 2024?\"\n",
        "    answer = await analyst.answer_earthquake_question(\n",
        "        question,\n",
        "        {\n",
        "            \"start_date\": \"2024-01-01\",\n",
        "            \"end_date\": \"2024-03-31\",\n",
        "            \"ML_min\": 4.0\n",
        "        }\n",
        "    )\n",
        "    print(f\"Q: {question}\")\n",
        "    print(f\"A: {answer}\")\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    return analyst  # Return analyst for interactive use\n",
        "\n",
        "def run_main():\n",
        "    \"\"\"Run main function with proper event loop handling\"\"\"\n",
        "    try:\n",
        "        # Check if we're in a running event loop (like Jupyter)\n",
        "        loop = asyncio.get_running_loop()\n",
        "        if loop and loop.is_running():\n",
        "            # We're in a running event loop, create a task instead\n",
        "            import nest_asyncio\n",
        "            nest_asyncio.apply()\n",
        "            return asyncio.run(main())\n",
        "        else:\n",
        "            return asyncio.run(main())\n",
        "    except RuntimeError:\n",
        "        # Fallback: try to run in existing loop\n",
        "        try:\n",
        "            import nest_asyncio\n",
        "            nest_asyncio.apply()\n",
        "            return asyncio.run(main())\n",
        "        except ImportError:\n",
        "            print(\"For Jupyter notebook, please install nest_asyncio: pip install nest_asyncio\")\n",
        "            print(\"Or use the notebook-friendly functions below:\")\n",
        "            return None\n",
        "\n",
        "# Notebook-friendly wrapper functions\n",
        "async def quick_analysis(api_key: str,\n",
        "                        start_date: str = \"2024-01-01\",\n",
        "                        end_date: str = \"2024-12-31\",\n",
        "                        ml_min: float = 4.0):\n",
        "    \"\"\"Quick analysis function for notebook use\"\"\"\n",
        "    analyst = GeminiEarthquakeAnalyst(api_key)\n",
        "    await analyst.connect_to_mcp_server(\"https://cwadayi-mcp-2.hf.space\")\n",
        "\n",
        "    if await analyst.initialize_connection():\n",
        "        analysis = await analyst.analyze_earthquake_data({\n",
        "            \"start_date\": start_date,\n",
        "            \"end_date\": end_date,\n",
        "            \"ML_min\": ml_min\n",
        "        })\n",
        "        return analysis\n",
        "    return \"Failed to connect to MCP server\"\n",
        "\n",
        "async def ask_earthquake_question(api_key: str,\n",
        "                                question: str,\n",
        "                                start_date: str = \"2024-01-01\",\n",
        "                                end_date: str = \"2024-12-31\",\n",
        "                                ml_min: float = 4.0):\n",
        "    \"\"\"Ask a question about earthquakes\"\"\"\n",
        "    analyst = GeminiEarthquakeAnalyst(api_key)\n",
        "    await analyst.connect_to_mcp_server(\"https://cwadayi-mcp-2.hf.space\")\n",
        "\n",
        "    if await analyst.initialize_connection():\n",
        "        answer = await analyst.answer_earthquake_question(\n",
        "            question,\n",
        "            {\n",
        "                \"start_date\": start_date,\n",
        "                \"end_date\": end_date,\n",
        "                \"ML_min\": ml_min\n",
        "            }\n",
        "        )\n",
        "        return answer\n",
        "    return \"Failed to connect to MCP server\"\n",
        "\n",
        "async def create_analyst(api_key: str):\n",
        "    \"\"\"Create and initialize analyst for interactive use\"\"\"\n",
        "    analyst = GeminiEarthquakeAnalyst(api_key)\n",
        "    await analyst.connect_to_mcp_server(\"https://cwadayi-mcp-2.hf.space\")\n",
        "\n",
        "    if await analyst.initialize_connection():\n",
        "        return analyst\n",
        "    return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Try to run main, handle event loop issues\n",
        "    analyst = run_main()\n",
        "\n",
        "    # If main didn't run due to event loop issues, provide alternative\n",
        "    if analyst is None:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"EVENT LOOP ISSUE DETECTED\")\n",
        "        print(\"=\"*60)\n",
        "        print(\"If you're in Jupyter notebook, use these commands instead:\")\n",
        "        print()\n",
        "        print(\"# Set your API key\")\n",
        "        print(\"api_key = 'your-gemini-api-key'\")\n",
        "        print()\n",
        "        print(\"# Quick analysis\")\n",
        "        print(\"analysis = await quick_analysis(api_key)\")\n",
        "        print(\"print(analysis)\")\n",
        "        print()\n",
        "        print(\"# Ask a question\")\n",
        "        print(\"answer = await ask_earthquake_question(api_key, 'What was the strongest earthquake?')\")\n",
        "        print(\"print(answer)\")\n",
        "        print()\n",
        "        print(\"# Create analyst for multiple queries\")\n",
        "        print(\"analyst = await create_analyst(api_key)\")\n",
        "        print(\"if analyst:\")\n",
        "        print(\"    result = await analyst.analyze_earthquake_data({'ML_min': 5.0})\")\n",
        "        print(\"    print(result)\")\n",
        "        print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsPHSZLiWToTCuPaqnL4ul",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}